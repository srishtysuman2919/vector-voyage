{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('/Users/srishtysuman/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from llama_index.schema import Document\n",
    "from llama_index.evaluation import generate_question_context_pairs\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "import random\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pandas as pd\n",
    "from llama_index.evaluation import RetrieverEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page: {url}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Finding all 'a' tags which typically contain href attribute for links\n",
    "    links = [ urljoin(url, a[\"href\"]) for a in soup.find_all(\"a\", href=True) if a[\"href\"] ]\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 10/10 [00:05<00:00,  1.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id_='454bec35-eeb8-46d6-af40-3e2923bccc16', embedding=None, metadata={'source': 'https://docs.deeplake.ai/en/latest/', 'title': 'Deep Lake API Reference — Deep Lake 3.8.20 documentation', 'language': 'en'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='latest\\n\\nGetting Started\\n\\n  * Installation\\n\\nKey Concepts\\n\\n  * Datasets\\n  * Vector Store\\n  * Tensors\\n  * Htypes\\n  * Compressions\\n  * PyTorch and Tensorflow Support\\n  * Utility Functions\\n\\nIntegrations\\n\\n  * Weights and Biases\\n  * MMDetection\\n\\nHigh-Performance Features\\n\\n  * Dataloader\\n  * Sampler\\n  * Tensor Query Language\\n  * Random Split\\n  * Deep Memory\\n\\nAPI Reference\\n\\n  * deeplake\\n  * deeplake.VectorStore\\n  * deeplake.core\\n  * deeplake.core.dataset\\n  * deeplake.core.tensor\\n  * deeplake.api\\n  * deeplake.auto\\n  * deeplake.util\\n  * deeplake.client.log\\n  * deeplake.core.transform\\n  * deeplake.core.vectorstore.deep_memory\\n  * deeplake.random.seed\\n\\n__Deep Lake\\n\\n  * »\\n  * Deep Lake API Reference\\n  * Edit on GitHub\\n\\n* * *\\n\\n# Deep Lake API Reference\\uf0c1\\n\\nDeep Lake is an open-source database for AI.\\n\\nGetting Started\\n\\n  * Installation\\n\\nKey Concepts\\n\\n  * Datasets\\n    * Creating Datasets\\n    * Loading Datasets\\n    * Deleting and Renaming Datasets\\n    * Copying Datasets\\n    * Dataset Operations\\n    * Dataset Visualization\\n    * Dataset Credentials\\n    * Dataset Properties\\n    * Dataset Version Control\\n    * Dataset Views\\n  * Vector Store\\n    * Creating a Deep Lake Vector Store\\n    * Vector Store Operations\\n    * Vector Store Properties\\n  * Tensors\\n    * Creating Tensors\\n    * Deleting and Renaming Tensors\\n    * Adding and deleting samples\\n    * Retrieving samples\\n    * Tensor Properties\\n    * Info\\n    * Video features\\n  * Htypes\\n    * Image Htype\\n    * Video Htype\\n    * Audio Htype\\n    * Class Label Htype\\n    * Tag Htype\\n    * Bounding Box Htype\\n    * 3D Bounding Box Htype\\n    * Intrinsics Htype\\n    * Segmentation Mask Htype\\n    * Binary Mask Htype\\n    * COCO Keypoints Htype\\n    * Point Htype\\n    * Polygon Htype\\n    * Nifti Htype\\n    * Point Cloud Htype\\n    * Mesh Htype\\n    * Embedding Htype\\n    * Sequence htype\\n    * Link htype\\n  * Compressions\\n    * Sample Compression\\n    * Chunk Compression\\n  * PyTorch and Tensorflow Support\\n  * Utility Functions\\n    * General Functions\\n    * Making Deep Lake Samples\\n    * Parallelism\\n\\nIntegrations\\n\\n  * Weights and Biases\\n    * Logging Dataset Creation\\n    * Logging Dataset Read\\n  * MMDetection\\n\\nHigh-Performance Features\\n\\n  * Dataloader\\n  * Sampler\\n  * Tensor Query Language\\n  * Random Split\\n  * Deep Memory\\n\\nAPI Reference\\n\\n  * deeplake\\n  * deeplake.VectorStore\\n  * deeplake.core\\n  * deeplake.core.dataset\\n  * deeplake.core.tensor\\n  * deeplake.api\\n  * deeplake.auto\\n  * deeplake.util\\n  * deeplake.client.log\\n  * deeplake.core.transform\\n  * deeplake.core.vectorstore.deep_memory\\n  * deeplake.random.seed\\n\\n# Indices and tables\\uf0c1\\n\\n  * Index\\n\\n  * Module Index\\n\\n  * Search Page\\n\\nNext\\n\\n* * *\\n\\n(C) Copyright 2022, Activeloop. Revision `db792667`.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n\\nRead the Docs v: latest\\n\\nVersions\\n\\n    latest\\n    v3.1.5\\n    v3.1.0\\n    v3.0.16\\n    v3.0.15\\n    v2.8.5\\n\\nDownloads\\n\\n    pdf\\n    epub\\n\\nOn Read the Docs\\n\\n     Project Home\\n     Builds\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e87a3b4d-97b8-4b38-9582-8a8fc2d1ac12', embedding=None, metadata={'source': 'https://docs.deeplake.ai/en/latest/Installation.html', 'title': 'Installation — Deep Lake 3.8.20 documentation', 'language': 'en'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nlatest\\n\\nGetting Started\\n\\n  * Installation\\n\\nKey Concepts\\n\\n  * Datasets\\n  * Vector Store\\n  * Tensors\\n  * Htypes\\n  * Compressions\\n  * PyTorch and Tensorflow Support\\n  * Utility Functions\\n\\nIntegrations\\n\\n  * Weights and Biases\\n  * MMDetection\\n\\nHigh-Performance Features\\n\\n  * Dataloader\\n  * Sampler\\n  * Tensor Query Language\\n  * Random Split\\n  * Deep Memory\\n\\nAPI Reference\\n\\n  * deeplake\\n  * deeplake.VectorStore\\n  * deeplake.core\\n  * deeplake.core.dataset\\n  * deeplake.core.tensor\\n  * deeplake.api\\n  * deeplake.auto\\n  * deeplake.util\\n  * deeplake.client.log\\n  * deeplake.core.transform\\n  * deeplake.core.vectorstore.deep_memory\\n  * deeplake.random.seed\\n\\n__Deep Lake\\n\\n  * »\\n  * Installation\\n  * Edit on GitHub\\n\\n* * *\\n\\n# Installation\\uf0c1\\n\\nDeep Lake can be installed with pip\\n\\n    \\n    \\n    pip install deeplake\\n    \\n\\nDeep Lake has the following extras that you can choose to install according to\\nyour needs.\\n\\nInstallation commands\\uf0c1\\n\\nInstall command\\n\\n|\\n\\nDescription\\n\\n|\\n\\nDependencies installed  \\n  \\n---|---|---  \\n  \\n`pip install \"deeplake[av]\"`\\n\\n|\\n\\nAudio and video support via PyAV\\n\\n|\\n\\nav  \\n  \\n`pip install \"deeplake[visualizer]\"`\\n\\n|\\n\\nVisualize Deep Lake datasets within notebooks. This is required for\\n`Dataset.visualize` to work.\\n\\n|\\n\\nIPython, flask  \\n  \\n`pip install \"deeplake[gcp]\"`\\n\\n|\\n\\nGCS support\\n\\n|\\n\\ngoogle-cloud-storage, google-auth, google-auth-oauthlib  \\n  \\n`pip install \"deeplake[azure]\"`\\n\\n|\\n\\nAzure Blob Storage support\\n\\n|\\n\\nazure-storage-blob, azure-cli, azure-identity  \\n  \\n`pip install \"deeplake[medical]\"`\\n\\n|\\n\\nDICOM and NIfTI data support\\n\\n|\\n\\npydicom, nibabel  \\n  \\n`pip install \"deeplake[gdrive]\"`\\n\\n|\\n\\nGoogle Drive support\\n\\n|\\n\\ngoogle-api-python-client, oauth2client, google-auth, google-auth-oauthlib  \\n  \\n`pip install \"deeplake[point_cloud]\"`\\n\\n|\\n\\nSupport for LiDAR point cloud data\\n\\n|\\n\\nlaspy  \\n  \\n`pip install \"deeplake[all]\"`\\n\\n|\\n\\nInstalls all of the above\\n\\n|  \\n  \\nPrevious Next\\n\\n* * *\\n\\n(C) Copyright 2022, Activeloop. Revision `db792667`.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n\\nRead the Docs v: latest\\n\\nVersions\\n\\n    latest\\n    v3.1.5\\n    v3.1.0\\n    v3.0.16\\n    v3.0.15\\n    v2.8.5\\n\\nDownloads\\n\\n    pdf\\n    epub\\n\\nOn Read the Docs\\n\\n     Project Home\\n     Builds\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1fdf019f-2ba5-4cc0-acb8-696408846de5', embedding=None, metadata={'source': 'https://docs.deeplake.ai/en/latest/Datasets.html', 'title': 'Datasets — Deep Lake 3.8.20 documentation', 'language': 'en'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nlatest\\n\\nGetting Started\\n\\n  * Installation\\n\\nKey Concepts\\n\\n  * Datasets\\n    * Creating Datasets\\n    * Loading Datasets\\n    * Deleting and Renaming Datasets\\n    * Copying Datasets\\n    * Dataset Operations\\n    * Dataset Visualization\\n    * Dataset Credentials\\n    * Dataset Properties\\n    * Dataset Version Control\\n    * Dataset Views\\n  * Vector Store\\n  * Tensors\\n  * Htypes\\n  * Compressions\\n  * PyTorch and Tensorflow Support\\n  * Utility Functions\\n\\nIntegrations\\n\\n  * Weights and Biases\\n  * MMDetection\\n\\nHigh-Performance Features\\n\\n  * Dataloader\\n  * Sampler\\n  * Tensor Query Language\\n  * Random Split\\n  * Deep Memory\\n\\nAPI Reference\\n\\n  * deeplake\\n  * deeplake.VectorStore\\n  * deeplake.core\\n  * deeplake.core.dataset\\n  * deeplake.core.tensor\\n  * deeplake.api\\n  * deeplake.auto\\n  * deeplake.util\\n  * deeplake.client.log\\n  * deeplake.core.transform\\n  * deeplake.core.vectorstore.deep_memory\\n  * deeplake.random.seed\\n\\n__Deep Lake\\n\\n  * »\\n  * Datasets\\n  * Edit on GitHub\\n\\n* * *\\n\\n# Datasets\\uf0c1\\n\\n## Creating Datasets\\uf0c1\\n\\n`deeplake.dataset`\\n\\n|\\n\\nReturns a `Dataset` object referencing either a new or existing dataset.  \\n  \\n---|---  \\n  \\n`deeplake.empty`\\n\\n|\\n\\nCreates an empty dataset  \\n  \\n`deeplake.like`\\n\\n|\\n\\nCreates a new dataset by copying the `source` dataset\\'s structure to a new\\nlocation.  \\n  \\n`deeplake.ingest_classification`\\n\\n|\\n\\nIngest a dataset of images from a local folder to a Deep Lake Dataset.  \\n  \\n`deeplake.ingest_coco`\\n\\n|\\n\\nIngest images and annotations in COCO format to a Deep Lake Dataset.  \\n  \\n`deeplake.ingest_yolo`\\n\\n|\\n\\nIngest images and annotations (bounding boxes or polygons) in YOLO format to a\\nDeep Lake Dataset.  \\n  \\n`deeplake.ingest_kaggle`\\n\\n|\\n\\nDownload and ingest a kaggle dataset and store it as a structured dataset to\\ndestination.  \\n  \\n`deeplake.ingest_dataframe`\\n\\n|\\n\\nConvert pandas dataframe to a Deep Lake Dataset.  \\n  \\n`deeplake.ingest_huggingface`\\n\\n|\\n\\nConverts Hugging Face datasets to Deep Lake format.  \\n  \\n## Loading Datasets\\uf0c1\\n\\n`deeplake.load`\\n\\n|\\n\\nLoads an existing dataset  \\n  \\n---|---  \\n  \\n## Deleting and Renaming Datasets\\uf0c1\\n\\n`deeplake.delete`\\n\\n|\\n\\nDeletes a dataset at a given path.  \\n  \\n---|---  \\n  \\n`deeplake.rename`\\n\\n|\\n\\nRenames dataset at `old_path` to `new_path`.  \\n  \\n## Copying Datasets\\uf0c1\\n\\n`deeplake.copy`\\n\\n|\\n\\nCopies dataset at `src` to `dest`.  \\n  \\n---|---  \\n  \\n`deeplake.deepcopy`\\n\\n|\\n\\nCopies dataset at `src` to `dest` including version control history.  \\n  \\n## Dataset Operations\\uf0c1\\n\\n`Dataset.summary`\\n\\n|\\n\\nPrints a summary of the dataset.  \\n  \\n---|---  \\n  \\n`Dataset.append`\\n\\n|\\n\\nAppend samples to mutliple tensors at once.  \\n  \\n`Dataset.extend`\\n\\n|\\n\\nAppends multiple rows of samples to mutliple tensors at once.  \\n  \\n`Dataset.update`\\n\\n|\\n\\nUpdate existing samples in the dataset with new values.  \\n  \\n`Dataset.query`\\n\\n|\\n\\nReturns a sliced `Dataset` with given query results.  \\n  \\n`Dataset.copy`\\n\\n|\\n\\nCopies this dataset or dataset view to `dest`.  \\n  \\n`Dataset.delete`\\n\\n|\\n\\nDeletes the entire dataset from the cache layers (if any) and the underlying\\nstorage.  \\n  \\n`Dataset.rename`\\n\\n|\\n\\nRenames the dataset to path.  \\n  \\n`Dataset.connect`\\n\\n|\\n\\nConnect a Deep Lake cloud dataset through a deeplake path.  \\n  \\n`Dataset.visualize`\\n\\n|\\n\\nVisualizes the dataset in the Jupyter notebook.  \\n  \\n`Dataset.pop`\\n\\n|\\n\\nRemoves a sample from all the tensors of the dataset.  \\n  \\n`Dataset.rechunk`\\n\\n|\\n\\nRewrites the underlying chunks to make their sizes optimal.  \\n  \\n`Dataset.flush`\\n\\n|\\n\\nNecessary operation after writes if caches are being used.  \\n  \\n`Dataset.clear_cache`\\n\\n|\\n\\n  * Flushes (see `Dataset.flush()`) the contents of the cache layers (if any) and then deletes contents of all the layers of it.\\n\\n  \\n  \\n`Dataset.size_approx`\\n\\n|\\n\\nEstimates the size in bytes of the dataset.  \\n  \\n`Dataset.random_split`\\n\\n|\\n\\nSplits the dataset into non-overlapping `Dataset` objects of given lengths.  \\n  \\n## Dataset Visualization\\uf0c1\\n\\n`Dataset.visualize`\\n\\n|\\n\\nVisualizes the dataset in the Jupyter notebook.  \\n  \\n---|---  \\n  \\n## Dataset Credentials\\uf0c1\\n\\n`Dataset.add_creds_key`\\n\\n|\\n\\nAdds a new creds key to the dataset.  \\n  \\n---|---  \\n  \\n`Dataset.populate_creds`\\n\\n|\\n\\nPopulates the creds key added in add_creds_key with the given creds.  \\n  \\n`Dataset.update_creds_key`\\n\\n|\\n\\nUpdates the name and/or management status of a creds key.  \\n  \\n`Dataset.get_creds_keys`\\n\\n|\\n\\nReturns the set of creds keys added to the dataset.  \\n  \\n## Dataset Properties\\uf0c1\\n\\n`Dataset.tensors`\\n\\n|\\n\\nAll tensors belonging to this group, including those within sub groups.  \\n  \\n---|---  \\n  \\n`Dataset.groups`\\n\\n|\\n\\nAll sub groups in this group  \\n  \\n`Dataset.num_samples`\\n\\n|\\n\\nReturns the length of the smallest tensor.  \\n  \\n`Dataset.read_only`\\n\\n|\\n\\nReturns True if dataset is in read-only mode and False otherwise.  \\n  \\n`Dataset.info`\\n\\n|\\n\\nReturns the information about the dataset.  \\n  \\n`Dataset.max_len`\\n\\n|\\n\\nReturn the maximum length of the tensor.  \\n  \\n`Dataset.min_len`\\n\\n|\\n\\nReturn the minimum length of the tensor.  \\n  \\n## Dataset Version Control\\uf0c1\\n\\n`Dataset.commit`\\n\\n|\\n\\nStores a snapshot of the current state of the dataset.  \\n  \\n---|---  \\n  \\n`Dataset.diff`\\n\\n|\\n\\nReturns/displays the differences between commits/branches.  \\n  \\n`Dataset.checkout`\\n\\n|\\n\\nChecks out to a specific commit_id or branch.  \\n  \\n`Dataset.merge`\\n\\n|\\n\\nMerges the target_id into the current dataset.  \\n  \\n`Dataset.log`\\n\\n|\\n\\nDisplays the details of all the past commits.  \\n  \\n`Dataset.reset`\\n\\n|\\n\\nResets the uncommitted changes present in the branch.  \\n  \\n`Dataset.get_commit_details`\\n\\n|\\n\\nGet details of a particular commit.  \\n  \\n`Dataset.commit_id`\\n\\n|\\n\\nThe lasted committed commit id of the dataset.  \\n  \\n`Dataset.branch`\\n\\n|\\n\\nThe current branch of the dataset  \\n  \\n`Dataset.pending_commit_id`\\n\\n|\\n\\nThe commit_id of the next commit that will be made to the dataset.  \\n  \\n`Dataset.has_head_changes`\\n\\n|\\n\\nReturns True if currently at head node and uncommitted changes are present.  \\n  \\n`Dataset.commits`\\n\\n|\\n\\nLists all the commits leading to the current dataset state.  \\n  \\n`Dataset.branches`\\n\\n|\\n\\nLists all the branches of the dataset.  \\n  \\n## Dataset Views\\uf0c1\\n\\nA dataset view is a subset of a dataset that points to specific samples\\n(indices) in an existing dataset. Dataset views can be created by indexing a\\ndataset, filtering a dataset with `Dataset.filter()`, querying a dataset with\\n`Dataset.query()` or by sampling a dataset with `Dataset.sample_by()`.\\nFiltering is done with user-defined functions or simplified expressions\\nwhereas query can perform SQL-like queries with our Tensor Query Language. See\\nthe full TQL spec here.\\n\\nDataset views can only be saved when a dataset has been committed and has no\\nchanges on the HEAD node, in order to preserve data lineage and prevent the\\nunderlying data from changing after the query or filter conditions have been\\nevaluated.\\n\\n**Example**\\n\\n    \\n    \\n    >>> import deeplake\\n    >>> # load dataset\\n    >>> ds = deeplake.load(\"hub://activeloop/mnist-train\")\\n    >>> # filter dataset\\n    >>> zeros = ds.filter(\"labels == 0\")\\n    >>> # save view\\n    >>> zeros.save_view(id=\"zeros\")\\n    >>> # load_view\\n    >>> zeros = ds.load_view(id=\"zeros\")\\n    >>> len(zeros)\\n    5923\\n    \\n\\n`Dataset.query`\\n\\n|\\n\\nReturns a sliced `Dataset` with given query results.  \\n  \\n---|---  \\n  \\n`Dataset.sample_by`\\n\\n|\\n\\nReturns a sliced `Dataset` with given weighted sampler applied.  \\n  \\n`Dataset.filter`\\n\\n|\\n\\nFilters the dataset in accordance of filter function `f(x: sample) -> bool`  \\n  \\n`Dataset.save_view`\\n\\n|\\n\\nSaves a dataset view as a virtual dataset (VDS)  \\n  \\n`Dataset.get_view`\\n\\n|\\n\\nReturns the dataset view corresponding to `id`.  \\n  \\n`Dataset.load_view`\\n\\n|\\n\\nLoads the view and returns the `Dataset` by id.  \\n  \\n`Dataset.delete_view`\\n\\n|\\n\\nDeletes the view with given view id.  \\n  \\n`Dataset.get_views`\\n\\n|\\n\\nReturns list of views stored in this Dataset.  \\n  \\n`Dataset.is_view`\\n\\n|\\n\\nReturns `True` if this dataset is a view and `False` otherwise.  \\n  \\n`Dataset.min_view`\\n\\n|\\n\\nReturns a view of the dataset in which all tensors are sliced to have the same\\nlength as the shortest tensor.  \\n  \\n`Dataset.max_view`\\n\\n|\\n\\nReturns a view of the dataset in which shorter tensors are padded with `None`\\ns to have the same length as the longest tensor.  \\n  \\nPrevious Next\\n\\n* * *\\n\\n(C) Copyright 2022, Activeloop. Revision `db792667`.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n\\nRead the Docs v: latest\\n\\nVersions\\n\\n    latest\\n    v3.1.5\\n    v3.1.0\\n    v3.0.16\\n    v3.0.15\\n    v2.8.5\\n\\nDownloads\\n\\n    pdf\\n    epub\\n\\nOn Read the Docs\\n\\n     Project Home\\n     Builds\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1d5722f4-d877-43cd-8414-7a690e9ce4a0', embedding=None, metadata={'source': 'https://docs.deeplake.ai/en/latest/Vector-Store.html', 'title': 'Vector Store — Deep Lake 3.8.20 documentation', 'language': 'en'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nlatest\\n\\nGetting Started\\n\\n  * Installation\\n\\nKey Concepts\\n\\n  * Datasets\\n  * Vector Store\\n    * Creating a Deep Lake Vector Store\\n    * Vector Store Operations\\n    * Vector Store Properties\\n  * Tensors\\n  * Htypes\\n  * Compressions\\n  * PyTorch and Tensorflow Support\\n  * Utility Functions\\n\\nIntegrations\\n\\n  * Weights and Biases\\n  * MMDetection\\n\\nHigh-Performance Features\\n\\n  * Dataloader\\n  * Sampler\\n  * Tensor Query Language\\n  * Random Split\\n  * Deep Memory\\n\\nAPI Reference\\n\\n  * deeplake\\n  * deeplake.VectorStore\\n  * deeplake.core\\n  * deeplake.core.dataset\\n  * deeplake.core.tensor\\n  * deeplake.api\\n  * deeplake.auto\\n  * deeplake.util\\n  * deeplake.client.log\\n  * deeplake.core.transform\\n  * deeplake.core.vectorstore.deep_memory\\n  * deeplake.random.seed\\n\\n__Deep Lake\\n\\n  * »\\n  * Vector Store\\n  * Edit on GitHub\\n\\n* * *\\n\\n# Vector Store\\uf0c1\\n\\n## Creating a Deep Lake Vector Store\\uf0c1\\n\\n`VectorStore.__init__`\\n\\n|\\n\\nCreates an empty VectorStore or loads an existing one if it exists at the\\nspecified `path`.  \\n  \\n---|---  \\n  \\n## Vector Store Operations\\uf0c1\\n\\n`VectorStore.add`\\n\\n|\\n\\nAdding elements to deeplake vector store.  \\n  \\n---|---  \\n  \\n`VectorStore.search`\\n\\n|\\n\\nVectorStore search method that combines embedding search, metadata search, and\\ncustom TQL search.  \\n  \\n`VectorStore.delete`\\n\\n|\\n\\nDelete the data in the Vector Store.  \\n  \\n`VectorStore.delete_by_path`\\n\\n|\\n\\nDeleted the Vector Store at the specified path.  \\n  \\n`VectorStore.update_embedding`\\n\\n|\\n\\nRecompute existing embeddings of the VectorStore, that match either query,\\nfilter, ids or row_ids.  \\n  \\n## Vector Store Properties\\uf0c1\\n\\n`VectorStore.summary`\\n\\n|\\n\\nPrints a summary of the dataset  \\n  \\n---|---  \\n  \\n`VectorStore.tensors`\\n\\n|\\n\\nReturns the list of tensors present in the dataset  \\n  \\n`VectorStore.__len__`\\n\\n|\\n\\nLength of the dataset  \\n  \\nPrevious Next\\n\\n* * *\\n\\n(C) Copyright 2022, Activeloop. Revision `db792667`.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n\\nRead the Docs v: latest\\n\\nVersions\\n\\n    latest\\n    v3.1.5\\n    v3.1.0\\n    v3.0.16\\n    v3.0.15\\n    v2.8.5\\n\\nDownloads\\n\\n    pdf\\n    epub\\n\\nOn Read the Docs\\n\\n     Project Home\\n     Builds\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6895b645-2029-44cf-b89d-88c48c4b9bfe', embedding=None, metadata={'source': 'https://docs.deeplake.ai/en/latest/Tensors.html', 'title': 'Tensors — Deep Lake 3.8.20 documentation', 'language': 'en'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nlatest\\n\\nGetting Started\\n\\n  * Installation\\n\\nKey Concepts\\n\\n  * Datasets\\n  * Vector Store\\n  * Tensors\\n    * Creating Tensors\\n    * Deleting and Renaming Tensors\\n    * Adding and deleting samples\\n    * Retrieving samples\\n    * Tensor Properties\\n    * Info\\n    * Video features\\n  * Htypes\\n  * Compressions\\n  * PyTorch and Tensorflow Support\\n  * Utility Functions\\n\\nIntegrations\\n\\n  * Weights and Biases\\n  * MMDetection\\n\\nHigh-Performance Features\\n\\n  * Dataloader\\n  * Sampler\\n  * Tensor Query Language\\n  * Random Split\\n  * Deep Memory\\n\\nAPI Reference\\n\\n  * deeplake\\n  * deeplake.VectorStore\\n  * deeplake.core\\n  * deeplake.core.dataset\\n  * deeplake.core.tensor\\n  * deeplake.api\\n  * deeplake.auto\\n  * deeplake.util\\n  * deeplake.client.log\\n  * deeplake.core.transform\\n  * deeplake.core.vectorstore.deep_memory\\n  * deeplake.random.seed\\n\\n__Deep Lake\\n\\n  * »\\n  * Tensors\\n  * Edit on GitHub\\n\\n* * *\\n\\n# Tensors\\uf0c1\\n\\n## Creating Tensors\\uf0c1\\n\\n`Dataset.create_tensor`\\n\\n|\\n\\nCreates a new tensor in the dataset.  \\n  \\n---|---  \\n  \\n`Dataset.create_group`\\n\\n|\\n\\nCreates a tensor group.  \\n  \\n`Dataset.create_tensor_like`\\n\\n|\\n\\nCopies the `source` tensor's meta information and creates a new tensor with\\nit.  \\n  \\n## Deleting and Renaming Tensors\\uf0c1\\n\\n`Dataset.delete_tensor`\\n\\n|\\n\\nDelete a tensor from the dataset.  \\n  \\n---|---  \\n  \\n`Dataset.delete_group`\\n\\n|\\n\\nDelete a tensor group from the dataset.  \\n  \\n`Dataset.rename_tensor`\\n\\n|\\n\\nRenames tensor with name `name` to `new_name`  \\n  \\n`Dataset.rename_group`\\n\\n|\\n\\nRenames group with name `name` to `new_name`  \\n  \\n## Adding and deleting samples\\uf0c1\\n\\n`Tensor.append`\\n\\n|\\n\\nAppends a single sample to the end of the tensor.  \\n  \\n---|---  \\n  \\n`Tensor.extend`\\n\\n|\\n\\nExtends the end of the tensor by appending multiple elements from a sequence.  \\n  \\n`Tensor.pop`\\n\\n|\\n\\nRemoves element(s) at the given index / indices.  \\n  \\n`Tensor.clear`\\n\\n|\\n\\nDeletes all samples from the tensor  \\n  \\n`Tensor.__setitem__`\\n\\n|\\n\\nUpdate samples with new values.  \\n  \\n## Retrieving samples\\uf0c1\\n\\n`Tensor.numpy`\\n\\n|\\n\\nComputes the contents of the tensor in numpy format.  \\n  \\n---|---  \\n  \\n`Tensor.data`\\n\\n|\\n\\nReturns data in the tensor in a format based on the tensor's base htype.  \\n  \\n`Tensor.tobytes`\\n\\n|\\n\\nReturns the bytes of the tensor.  \\n  \\n`Tensor.text`\\n\\n|\\n\\nReturn text data.  \\n  \\n`Tensor.dict`\\n\\n|\\n\\nReturn json data.  \\n  \\n`Tensor.list`\\n\\n|\\n\\nReturn list data.  \\n  \\n`Tensor._linked_sample`\\n\\n|\\n\\nReturns the linked sample at the given index.  \\n  \\n## Tensor Properties\\uf0c1\\n\\n`Tensor.htype`\\n\\n|\\n\\nHtype of the tensor.  \\n  \\n---|---  \\n  \\n`Tensor.base_htype`\\n\\n|\\n\\nBase htype of the tensor.  \\n  \\n`Tensor.dtype`\\n\\n|\\n\\nDtype of the tensor.  \\n  \\n`Tensor.shape`\\n\\n|\\n\\nGet the shape of this tensor.  \\n  \\n`Tensor.shape_interval`\\n\\n|\\n\\nReturns a `ShapeInterval` object that describes this tensor's shape more\\naccurately.  \\n  \\n`Tensor.ndim`\\n\\n|\\n\\nNumber of dimensions of the tensor.  \\n  \\n`Tensor.num_samples`\\n\\n|\\n\\nReturns the length of the primary axis of the tensor.  \\n  \\n`Tensor.__len__`\\n\\n|\\n\\nReturns the length of the primary axis of the tensor.  \\n  \\n`Tensor.is_dynamic`\\n\\n|\\n\\nWill return `True` if samples in this tensor have shapes that are unequal.  \\n  \\n`Tensor.is_sequence`\\n\\n|\\n\\nWhether this tensor is a sequence tensor.  \\n  \\n`Tensor.is_link`\\n\\n|\\n\\nWhether this tensor is a link tensor.  \\n  \\n`Tensor.verify`\\n\\n|\\n\\nWhether linked data will be verified when samples are added.  \\n  \\n## Info\\uf0c1\\n\\n`Tensor.info`\\n\\n|\\n\\nReturns the information about the tensor.  \\n  \\n---|---  \\n  \\n`Tensor.sample_info`\\n\\n|\\n\\nReturns info about particular samples in a tensor.  \\n  \\n## Video features\\uf0c1\\n\\n`Tensor.play`\\n\\n|\\n\\nPlay video sample.  \\n  \\n---|---  \\n  \\n`Tensor.timestamps`\\n\\n|\\n\\nReturns timestamps (in seconds) for video sample as numpy array.  \\n  \\nPrevious Next\\n\\n* * *\\n\\n(C) Copyright 2022, Activeloop. Revision `db792667`.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n\\nRead the Docs v: latest\\n\\nVersions\\n\\n    latest\\n    v3.1.5\\n    v3.1.0\\n    v3.0.16\\n    v3.0.15\\n    v2.8.5\\n\\nDownloads\\n\\n    pdf\\n    epub\\n\\nOn Read the Docs\\n\\n     Project Home\\n     Builds\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ae021e06-5e9c-4915-b247-3af0dd965af4', embedding=None, metadata={'source': 'https://docs.deeplake.ai/en/latest/Htypes.html', 'title': 'Htypes — Deep Lake 3.8.20 documentation', 'language': 'en'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nlatest\\n\\nGetting Started\\n\\n  * Installation\\n\\nKey Concepts\\n\\n  * Datasets\\n  * Vector Store\\n  * Tensors\\n  * Htypes\\n    * Image Htype\\n      * Creating an image tensor\\n      * Appending image samples\\n      * image.rgb and image.gray htypes\\n    * Video Htype\\n      * Creating a video tensor\\n      * Appending video samples\\n    * Audio Htype\\n      * Creating an audio tensor\\n      * Appending audio samples\\n    * Class Label Htype\\n      * Creating a class label tensor\\n      * Appending class labels\\n    * Tag Htype\\n      * Creating a tag tensor\\n      * Appending tag samples\\n    * Bounding Box Htype\\n      * Creating a bbox tensor\\n      * Appending bounding boxes\\n    * 3D Bounding Box Htype\\n      * Creating a 3d bbox tensor\\n      * Appending 3d bounding boxes\\n    * Intrinsics Htype\\n      * Creating an intrinsics tensor\\n      * Appending intrinsics matrices\\n    * Segmentation Mask Htype\\n      * Creating a segment_mask tensor\\n      * Appending segmentation masks\\n    * Binary Mask Htype\\n      * Creating a binary_mask tensor\\n      * Appending binary masks\\n    * COCO Keypoints Htype\\n      * Creating a keypoints_coco tensor\\n      * Appending keypoints\\n    * Point Htype\\n      * Creating a point tensor\\n      * Appending point samples\\n    * Polygon Htype\\n      * Creating a polygon tensor\\n      * Appending polygons\\n    * Nifti Htype\\n      * Creating a nifti tensor\\n      * Appending nifti data\\n    * Point Cloud Htype\\n      * Creating a point cloud tensor\\n      * Appending point clouds\\n    * Mesh Htype\\n      * Creating a mesh tensor\\n      * Appending meshes\\n    * Embedding Htype\\n      * Creating an embedding tensor\\n      * Appending embedding samples\\n    * Sequence htype\\n    * Link htype\\n  * Compressions\\n  * PyTorch and Tensorflow Support\\n  * Utility Functions\\n\\nIntegrations\\n\\n  * Weights and Biases\\n  * MMDetection\\n\\nHigh-Performance Features\\n\\n  * Dataloader\\n  * Sampler\\n  * Tensor Query Language\\n  * Random Split\\n  * Deep Memory\\n\\nAPI Reference\\n\\n  * deeplake\\n  * deeplake.VectorStore\\n  * deeplake.core\\n  * deeplake.core.dataset\\n  * deeplake.core.tensor\\n  * deeplake.api\\n  * deeplake.auto\\n  * deeplake.util\\n  * deeplake.client.log\\n  * deeplake.core.transform\\n  * deeplake.core.vectorstore.deep_memory\\n  * deeplake.random.seed\\n\\n__Deep Lake\\n\\n  * »\\n  * Htypes\\n  * Edit on GitHub\\n\\n* * *\\n\\n# Htypes\\uf0c1\\n\\nHtype is the class of a tensor: image, bounding box, generic tensor, etc.\\n\\nThe htype of a tensor can be specified at its creation\\n\\n    \\n    \\n    >>> ds.create_tensor(\"my_tensor\", htype=\"...\")\\n    \\n\\nIf not specified, the tensor’s htype defaults to “generic”.\\n\\nSpecifying an htype allows for strict settings and error handling, and it is\\ncritical for increasing the performance of Deep Lake datasets containing rich\\ndata such as images and videos.\\n\\nSupported htypes and their respective defaults are:\\n\\nHtype configs\\uf0c1\\n\\nHTYPE\\n\\n|\\n\\nDTYPE\\n\\n|\\n\\nCOMPRESSION  \\n  \\n---|---|---  \\n  \\ngeneric\\n\\n|\\n\\nNone\\n\\n|\\n\\nNone  \\n  \\nimage\\n\\n|\\n\\nuint8\\n\\n|\\n\\nRequired arg  \\n  \\nimage.rgb\\n\\n|\\n\\nuint8\\n\\n|\\n\\nRequired arg  \\n  \\nimage.gray\\n\\n|\\n\\nuint8\\n\\n|\\n\\nRequired arg  \\n  \\nvideo\\n\\n|\\n\\nuint8\\n\\n|\\n\\nRequired arg  \\n  \\naudio\\n\\n|\\n\\nfloat64\\n\\n|\\n\\nRequired arg  \\n  \\nclass_label\\n\\n|\\n\\nuint32\\n\\n|\\n\\nNone  \\n  \\ntag\\n\\n|\\n\\nstr\\n\\n|\\n\\nNone  \\n  \\nbbox\\n\\n|\\n\\nfloat32\\n\\n|\\n\\nNone  \\n  \\nbbox.3d\\n\\n|\\n\\nfloat32\\n\\n|\\n\\nNone  \\n  \\nintrinsics\\n\\n|\\n\\nfloat32\\n\\n|\\n\\nNone  \\n  \\nsegment_mask\\n\\n|\\n\\nuint32\\n\\n|\\n\\nNone  \\n  \\nbinary_mask\\n\\n|\\n\\nbool\\n\\n|\\n\\nNone  \\n  \\nkeypoints_coco\\n\\n|\\n\\nint32\\n\\n|\\n\\nNone  \\n  \\npoint\\n\\n|\\n\\nint32\\n\\n|\\n\\nNone  \\n  \\npolygon\\n\\n|\\n\\nfloat32\\n\\n|\\n\\nNone  \\n  \\ntext\\n\\n|\\n\\nstr\\n\\n|\\n\\nNone  \\n  \\njson\\n\\n|\\n\\nAny\\n\\n|\\n\\nNone  \\n  \\nlist\\n\\n|\\n\\nList\\n\\n|\\n\\nNone  \\n  \\ndicom\\n\\n|\\n\\nNone\\n\\n|\\n\\ndcm  \\n  \\nnifti\\n\\n|\\n\\nNone\\n\\n|\\n\\nRequired arg  \\n  \\npoint_cloud\\n\\n|\\n\\nNone\\n\\n|\\n\\nlas  \\n  \\nmesh\\n\\n|\\n\\nNone\\n\\n|\\n\\nply  \\n  \\ninstance_label\\n\\n|\\n\\nuint32\\n\\n|\\n\\nNone  \\n  \\nembedding\\n\\n|\\n\\nNone\\n\\n|\\n\\nNone  \\n  \\nlink\\n\\n|\\n\\nstr\\n\\n|\\n\\nNone  \\n  \\nsequence\\n\\n|\\n\\nNone\\n\\n|\\n\\nNone  \\n  \\n## Image Htype\\uf0c1\\n\\n  * Sample dimensions: `(height, width, # channels)` or `(height, width)`.\\n\\nImages can be stored in Deep Lake as compressed bytes or as raw arrays. Due to\\nthe high compression ratio for most image formats, it is highly recommended to\\nstore compressed images using the `sample_compression` input to the\\ncreate_tensor method.\\n\\n### Creating an image tensor\\uf0c1\\n\\nAn image tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"images\", htype=\"image\", sample_compression=\"jpg\")\\n    \\n\\nOR\\n\\n    \\n    \\n    >>> ds.create_tensor(\"images\", htype=\"image\", chunk_compression=\"jpg\")\\n    \\n\\n  * Optional args:\\n    \\n    * dtype: Defaults to `uint8`.\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [None, \"bmp\", \"dib\", \"gif\", \"ico\", \"jpeg\", \"jpeg2000\", \"pcx\", \"png\", \"ppm\", \"sgi\", \"tga\", \"tiff\",\\n    ... \"webp\", \"wmf\", \"xbm\", \"eps\", \"fli\", \"im\", \"msp\", \"mpo\"]\\n    \\n\\n### Appending image samples\\uf0c1\\n\\n  * Image samples can be of type `np.ndarray` or Deep Lake `Sample` which can be created using `deeplake.read()`.\\n\\nExamples\\n\\nAppending pixel data with array\\n\\n    \\n    \\n    >>> ds.images.append(np.zeros((5, 5, 3), dtype=np.uint8))\\n    \\n\\nAppening Deep Lake image sample\\n\\n    \\n    \\n    >>> ds.images.append(deeplake.read(\"images/0001.jpg\"))\\n    \\n\\nYou can append multiple samples at the same time using `extend()`.\\n\\n    \\n    \\n    >>> ds.images.extend([deeplake.read(f\"images/000{i}.jpg\") for i in range(10)])\\n    \\n\\nNote\\n\\nIf the compression format of the input sample does not match the\\n`sample_compression` of the tensor, Deep Lake will decompress and recompress\\nthe image for storage, which may significantly slow down the upload process.\\nThe upload process is fastest when the image compression matches the\\n`sample_compression`.\\n\\n### image.rgb and image.gray htypes\\uf0c1\\n\\n`image.rgb` and `image.gray` htypes can be used to force your samples to be of\\nRGB or grayscale type. i.e., if RGB images are appended to an `image.gray`\\ntensor, Deep Lake will convert them to grayscale and if grayscale images are\\nappended to an `image.rgb` tensor, Deep Lake will convert them to RGB format.\\n\\nimage.rgb and image.gray tensors can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"rgb_images\", htype=\"image.rgb\", sample_compression=\"...\")\\n    \\n    \\n    \\n    >>> ds.create_tensor(\"gray_images\", htype=\"image.gray\", sample_compression=\"...\")\\n    \\n\\n## Video Htype\\uf0c1\\n\\n  * Sample dimensions: `(# frames, height, width, # channels)` or `(# frames, height, width)`\\n\\n### Creating a video tensor\\uf0c1\\n\\nA video tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"videos\", htype=\"video\", sample_compression=\"mp4\")\\n    \\n\\n  * Optional args:\\n    \\n    * dtype: Defaults to `uint8`.\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [None, \"mp4\", \"mkv\", \"avi\"]\\n    \\n\\n### Appending video samples\\uf0c1\\n\\n  * Video samples can be of type `np.ndarray` or `Sample` which is returned by `deeplake.read()`.\\n\\n  * Deep Lake does not support compression of raw video frames. Therefore, array of raw frames can only be appended to tensors with `None` compression.\\n\\n  * Recompression of samples read with `deeplake.read` is also not supported.\\n\\nExamples\\n\\nAppending Deep Lake video sample\\n\\n    \\n    \\n    >>> ds.videos.append(deeplake.read(\"videos/0012.mp4\"))\\n    \\n\\nExtending with multiple videos\\n\\n    \\n    \\n    >>> ds.videos.extend([deeplake.read(f\"videos/00{i}.mp4\") for i in range(10)])\\n    \\n\\n## Audio Htype\\uf0c1\\n\\n  * Sample dimensions: `(# samples in audio, # channels)` or `(# samples in audio,)`\\n\\n### Creating an audio tensor\\uf0c1\\n\\nAn audio tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"audios\", htype=\"audio\", sample_compression=\"mp3\")\\n    \\n\\n  * Optional args:\\n    \\n    * dtype: Defaults to `float64`.\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [None, \"mp3\", \"wav\", \"flac\"]\\n    \\n\\n### Appending audio samples\\uf0c1\\n\\n  * Audio samples can be of type `np.ndarray` or `Sample` which is returned by `deeplake.read()`.\\n\\n  * Like videos, Deep Lake does not support compression or recompression of input audio samples. Thus, samples of type `np.ndarray` can only be appended to tensors with `None` compression.\\n\\nExamples\\n\\nAppending Deep Lake audio sample\\n\\n    \\n    \\n    >>> ds.audios.append(deeplake.read(\"audios/001.mp3\"))\\n    \\n\\nExtending with Deep Lake audio samples\\n\\n    \\n    \\n    >>> ds.audios.extend([deeplake.read(f\"videos/00{i}.mp3\") for i in range(10)])\\n    \\n\\n## Class Label Htype\\uf0c1\\n\\n  * Sample dimensions: `(# labels,)`\\n\\nClass labels are stored as numerical values in tensors, which are indices of\\nthe list `tensor.info.class_names`.\\n\\n### Creating a class label tensor\\uf0c1\\n\\nA class label tensor can be created using\\n\\n    \\n    \\n    >>> classes = [\"airplanes\", \"cars\", \"birds\", \"cats\", \"deer\", \"dogs\", \"frogs\", \"horses\", \"ships\", \"trucks\"]\\n    >>> ds.create_tensor(\"labels\", htype=\"class_label\", class_names=classes, chunk_compression=\"lz4\")\\n    \\n\\n  * Optional args:\\n    \\n    * class_names: This must be a **list of strings**. `tensor.info.class_names` will be set to this list.\\n\\n    * sample_compression or chunk_compression.\\n\\n    * dtype: Defaults to `uint32`.\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [\"lz4\"]\\n    \\n\\nYou can also choose to set the class names after tensor creation.\\n\\n    \\n    \\n    >>> ds.labels.info.update(class_names = [\"airplanes\", \"cars\", \"birds\", \"cats\", \"deer\", \"dogs\", \"frogs\", \"horses\", \"ships\", \"trucks\"])\\n    \\n\\nNote\\n\\nIf specifying compression, since the number of labels in one sample will be\\ntoo low, `chunk_compression` would be the better option to use.\\n\\n### Appending class labels\\uf0c1\\n\\n  * Class labels can be appended as `int`, `str`, `np.ndarray` or `list` of `int` or `str`.\\n\\n  * In case of strings, `tensor.info.class_names` is updated automatically.\\n\\nExamples\\n\\nAppending index\\n\\n    \\n    \\n    >>> ds.labels.append(0)\\n    >>> ds.labels.append(np.zeros((5,), dtype=np.uint32))\\n    \\n\\nExtending with list of indices\\n\\n    \\n    \\n    >>> ds.labels.extend([[0, 1, 2], [1, 3]])\\n    \\n\\nAppending text labels\\n\\n    \\n    \\n    >>> ds.labels.append([\"cars\", \"airplanes\"])\\n    \\n\\n## Tag Htype\\uf0c1\\n\\n  * Sample dimensions: `(# tags,)`\\n\\nThis htype can be used to tag samples with one or more string values.\\n\\n### Creating a tag tensor\\uf0c1\\n\\nA tag tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"tags\", htype=\"tag\", chunk_compression=\"lz4\")\\n    \\n\\n  * Optional args:\\n    \\n    * chunk_compression.\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [\"lz4\"]\\n    \\n\\n### Appending tag samples\\uf0c1\\n\\n  * Tag samples can be appended as `str` or `list` of `str`.\\n\\nExamples\\n\\nAppending a tag\\n\\n    \\n    \\n    >>> ds.tags.append(\"verified\")\\n    \\n\\nExtending with list of tags\\n\\n    \\n    \\n    >>> ds.tags.extend([\"verified\", \"unverified\"])\\n    \\n\\n## Bounding Box Htype\\uf0c1\\n\\n  * Sample dimensions: `(# bounding boxes, 4)`\\n\\nBounding boxes have a variety of conventions such as those used in YOLO, COCO,\\nPascal-VOC and others. In order for bounding boxes to be correctly displayed\\nby the visualizer, the format of the bounding box must be specified in the\\ncoords key in tensor meta information mentioned below.\\n\\n### Creating a bbox tensor\\uf0c1\\n\\nA bbox tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"boxes\", htype=\"bbox\", coords={\"type\": \"fractional\", \"mode\": \"CCWH\"})\\n    \\n\\n  * Optional args:\\n    \\n    * **coords** : A dictionary with keys “type” and “mode”.\\n    \\n      * **type** : Specifies the units of bounding box coordinates.\\n    \\n        * “pixel”: is in unit of pixels.\\n\\n        * “fractional”: is in units relative to the width and height of the image, such as in YOLO format.\\n\\n      * **mode** : Specifies the convention for the 4 coordinates\\n    \\n        * “LTRB”: left_x, top_y, right_x, bottom_y\\n\\n        * “LTWH”: left_x, top_y, width, height\\n\\n        * “CCWH”: center_x, center_y, width, height\\n\\n    * **dtype** : Defaults to `float32`.\\n\\n    * sample_compression or chunk_compression.\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [\"lz4\"]\\n    \\n\\nYou can also choose to set the class names after tensor creation.\\n\\n    \\n    \\n    >>> ds.boxes.info.update(coords = {\"type\": \"pixel\", \"mode\": \"LTRB\"})\\n    \\n\\nNote\\n\\nIf the bounding box format is not specified, the visualizer will assume a YOLO\\nformat (`fractional` \\\\+ `CCWH`) if the box coordinates are < 1 on average.\\nOtherwise, it will assume the COCO format (`pixel` \\\\+ `LTWH`).\\n\\n### Appending bounding boxes\\uf0c1\\n\\n  * Bounding boxes can be appended as `np.ndarrays` or `list` or `lists of arrays`.\\n\\nExamples\\n\\nAppending one bounding box\\n\\n    \\n    \\n    >>> box\\n    array([[462, 123, 238,  98]])\\n    >>> ds.boxes.append(box)\\n    \\n\\nAppending sample with 3 bounding boxes\\n\\n    \\n    \\n    >>> boxes\\n    array([[965, 110, 262,  77],\\n           [462, 123, 238,  98],\\n           [688, 108, 279, 116]])\\n    >>> boxes.shape\\n    (3, 4)\\n    >>> ds.boxes.append(boxes)\\n    \\n\\n## 3D Bounding Box Htype\\uf0c1\\n\\nIn order for 3D bounding boxes to be correctly displayed by the visualizer,\\nthe format of the bounding box must be specified in the coords key in tensor\\nmeta information mentioned below.\\n\\n### Creating a 3d bbox tensor\\uf0c1\\n\\nNote\\n\\nIn order for 3D bounding boxes to be correctly displayed by the visualizer,\\nthe format of the bounding box must be specified in the coords key in tensor\\nmeta information mentioned below. In addition, for projecting 3D bounding\\nboxes onto 2D data (such as an image), the intrinsics tensor must exist in the\\ndataset, or the intrinsics matrix must be specified in the\\n`ds.img_tensor.info` dictionary, where the key is `\"intrinsics\"` and the value\\nis the matrix.\\n\\nA 3d bbox tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"3d_boxes\", htype=\"bbox.3d\", coords={\"mode\": \"center\"})\\n    \\n\\n  * Optional args:\\n    \\n    * **coords** : A dictionary with key “mode”.\\n    \\n      * **mode** : Specifies the convention for the bbox coordinates.\\n    \\n        * “center”: [center_x, center_y, center_z, size_x, size_y, size_z, rot_x, rot_y, rot_z]\\n    \\n          * Sample dimensions: `(# bounding boxes, 9)`\\n\\n          * `size_x` \\\\- is the length of the bounding box along x direction\\n\\n          * `size_y` \\\\- is the width of the bounding box along y direction\\n\\n          * `size_z` \\\\- is the height of the bounding box along z direction\\n\\n          * `rot_x` \\\\- rotation angle along x axis, given in degrees\\n\\n          * `rot_y` \\\\- rotation angle along y axis, given in degrees\\n\\n          * `rot_z` \\\\- rotation angle along z axis, given in degrees\\n\\n        * “vertex”: 8 3D vertices - [[x0, y0, z0], [x1, y1, z1], [x2, y2, z2], ….., [x7, y7, z7]]\\n    \\n          * Sample dimensions: `(# bounding boxes, 8, 3)`\\n\\nThe vertex order is of the following form:\\n\\n                \\n                                      4_____________________ 5\\n                     /|                    /|\\n                    / |                   / |\\n                   /  |                  /  |\\n                  /___|_________________/   |\\n                0|    |                 | 1 |\\n                 |    |                 |   |\\n                 |    |                 |   |\\n                 |    |                 |   |\\n                 |    |_________________|___|\\n                 |   /  7               |   / 6\\n                 |  /                   |  /\\n                 | /                    | /\\n                 |/_____________________|/\\n                  3                      2\\n                \\n\\n    * **dtype** : Defaults to `float32`.\\n\\n    * sample_compression or chunk_compression.\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [\"lz4\"]\\n    \\n\\nNote\\n\\nrotation angles are specified in degrees, not radians\\n\\n### Appending 3d bounding boxes\\uf0c1\\n\\n  * Bounding boxes can be appended as `np.ndarrays` or `list` or `lists of arrays`.\\n\\nExamples\\n\\nAppending one bounding box\\n\\n    \\n    \\n    >>> box\\n    array([[462, 123, 238,  98, 22, 36, 44, 18, 0, 36, 0]])\\n    >>> ds.3d_boxes.append(box)\\n    \\n\\nAppending sample with 3 bounding boxes\\n\\n    \\n    \\n    >>> boxes\\n    array([[965, 110, 262,  77, 22, 36, 44, 18, 0, 28, 0],\\n           [462, 123, 238,  98, 26, 34, 24, 19, 0, -50, 0],\\n           [688, 108, 279, 116, 12, 32, 14, 38, 0, 30, 0]])\\n    >>> boxes.shape\\n    (9, 4)\\n    >>> ds.3d_boxes.append(boxes)\\n    \\n\\n## Intrinsics Htype\\uf0c1\\n\\n  * Sample dimensions: `(# intrinsics matrices, 3, 3)`\\n\\nThe intrinsic matrix represents a projective transformation from the 3-D\\ncamera’s coordinates into the 2-D image coordinates. The intrinsic parameters\\ninclude the focal length, the optical center, also known as the principal\\npoint. The camera intrinsic matrix, \\\\\\\\(K\\\\\\\\), is defined as:\\n\\n\\\\\\\\[\\\\begin{split}\\\\begin{bmatrix} f_x & 0 & c_x \\\\\\\\\\\\ 0 & f_y & c_y \\\\\\\\\\\\ 0 & 0 & 1\\n\\\\end{bmatrix}\\\\end{split}\\\\\\\\]\\n\\n  * \\\\\\\\([c_x, c_y]\\\\\\\\) \\\\- Optical center (the principal point), in pixels.\\n\\n  * \\\\\\\\([f_x, f_y]\\\\\\\\) \\\\- Focal length in pixels.\\n\\n  * \\\\\\\\(f_x = F / p_x\\\\\\\\)\\n\\n  * \\\\\\\\(f_y = F / p_y\\\\\\\\)\\n\\n  * \\\\\\\\(F\\\\\\\\) \\\\- Focal length in world units, typically expressed in millimeters.\\n\\n  * \\\\\\\\((p_x, p_y)\\\\\\\\) \\\\- Size of the pixel in world units.\\n\\n### Creating an intrinsics tensor\\uf0c1\\n\\nAn intrinsics tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"intrinsics\", htype=\"intrinsics\")\\n    \\n\\n  * Optional args:\\n    \\n    * sample_compression or chunk_compression.\\n\\n    * dtype: Defaults to `float32`.\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [\"lz4\"]\\n    \\n\\n### Appending intrinsics matrices\\uf0c1\\n\\n    \\n    \\n    >>> intrinsic_params = np.zeros((3, 3))\\n    >>> ds.intrinsics.append(intrinsic_params)\\n    \\n\\n## Segmentation Mask Htype\\uf0c1\\n\\n  * Sample dimensions: `(height, width)`\\n\\nSegmentation masks are 2D representations of class labels where the numerical\\nlabel data is encoded in an array of same shape as the image. The numerical\\nvalues are indices of the list `tensor.info.class_names`.\\n\\n### Creating a segment_mask tensor\\uf0c1\\n\\nA segment_mask tensor can be created using\\n\\n    \\n    \\n    >>> classes = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\"]\\n    >>> ds.create_tensor(\"masks\", htype=\"segment_mask\", class_names=classes, sample_compression=\"lz4\")\\n    \\n\\n  * Optional args:\\n    \\n    * class_names: This must be a **list of strings**. `tensor.info.class_names` will be set to this list.\\n\\n    * sample_compression or chunk_compression\\n\\n    * dtype: Defaults to `uint32`.\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [\"lz4\"]\\n    \\n\\nYou can also choose to set the class names after tensor creation.\\n\\n    \\n    \\n    >>> ds.labels.info.update(class_names = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\"])\\n    \\n\\nNote\\n\\nSince segmentation masks often contain large amounts of data, it is\\nrecommended to compress them using `lz4`.\\n\\n### Appending segmentation masks\\uf0c1\\n\\n  * Segmentation masks can be appended as `np.ndarray`.\\n\\nExamples\\n\\n    \\n    \\n    >>> ds.masks.append(np.zeros((512, 512)))\\n    \\n\\nNote\\n\\nSince each pixel can only be labeled once, segmentation masks are not\\nappropriate for datasets where objects might overlap, or where multiple\\nobjects within the same class must be distinguished. For these use cases,\\nplease use htype = “binary_mask”.\\n\\n## Binary Mask Htype\\uf0c1\\n\\n  * Sample dimensions: `(height, width, # objects in a sample)`\\n\\nBinary masks are similar to segmentation masks, except that each object is\\nrepresented by a channel in the mask. Each channel in the mask encodes values\\nfor a single object. A pixel in a mask channel should have a value of 1 if the\\npixel of the image belongs to this object and 0 otherwise. The labels\\ncorresponding to the channels should be stored in an adjacent tensor of htype\\n`class_label`, in which the number of labels at a given index is equal to the\\nnumber of objects (number of channels) in the binary mask.\\n\\n### Creating a binary_mask tensor\\uf0c1\\n\\nA binary_mask tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"masks\", htype=\"binary_mask\", sample_compression=\"lz4\")\\n    \\n\\n  * Optional args:\\n    \\n    * ref:sample_compression <sample_compression> or chunk_compression\\n\\n    * dtype: Defaults to `bool`.\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [\"lz4\"]\\n    \\n\\nNote\\n\\nSince segmentation masks often contain large amounts of data, it is\\nrecommended to compress them using `lz4`.\\n\\n### Appending binary masks\\uf0c1\\n\\n  * Binary masks can be appended as `np.ndarray`.\\n\\nExamples\\n\\nAppending a binary mask with 5 objects\\n\\n    \\n    \\n    >>> ds.masks.append(np.zeros((512, 512, 5), dtype=\"bool\"))\\n    >>> ds.labels.append([\"aeroplane\", \"aeroplane\", \"bottle\", \"bottle\", \"bird\"])\\n    \\n\\n## COCO Keypoints Htype\\uf0c1\\n\\n  * Sample dimensions: `(3 x # keypoints, # objects in a sample)`\\n\\nCOCO keypoints are a convention for storing points of interest in an image.\\nEach keypoint consists of 3 values: `x - coordinate`, `y - coordinate` and `v\\n- visibility`. A set of `K` keypoints of an object is represented as:\\n\\n[x1, y1, v1, x2, y2, v2, …, xk, yk, vk]\\n\\nThe visibility `v` can be one of three values:\\n\\n0\\n\\n    \\n\\nkeypoint not in image.\\n\\n1\\n\\n    \\n\\nkeypoint in image but not visible.\\n\\n2\\n\\n    \\n\\nkeypoint in image and visible.\\n\\n### Creating a keypoints_coco tensor\\uf0c1\\n\\nA keypoints_coco tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"keypoints\", htype=\"keypoints_coco\", keypoints=[\"knee\", \"elbow\", \"head\"], connections=[[0, 1], [1, 2]])\\n    \\n\\n  * Optional args:\\n    \\n    * keypoints: List of strings describing the `i` th keypoint. `tensor.info.keypoints` will be set to this list.\\n\\n    * connections: List of strings describing which points should be connected by lines in the visualizer.\\n\\n    * sample_compression or chunk_compression\\n\\n    * dtype: Defaults to `int32`.\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [\"lz4\"]\\n    \\n\\nYou can also choose to set `keypoints` and / or `connections` after tensor\\ncreation.\\n\\n    \\n    \\n    >>> ds.keypoints.info.update(keypoints = [\\'knee\\', \\'elbow\\',...])\\n    >>> ds.keypoints.info.update(connections = [[0,1], [2,3], ...])\\n    \\n\\n### Appending keypoints\\uf0c1\\n\\n  * Keypoints can be appended as `np.ndarray` or `list`.\\n\\nExamples\\n\\nAppending keypoints sample with 3 keypoints and 4 objects\\n\\n    \\n    \\n    >>> ds.keypoints.update(keypoints = [\"left ear\", \"right ear\", \"nose\"])\\n    >>> ds.keypoints.update(connections = [[0, 2], [1, 2]])\\n    >>> kp_arr\\n    array([[465, 398, 684, 469],\\n           [178, 363, 177, 177],\\n           [  2,   2,   2,   1],\\n           [454, 387, 646, 478],\\n           [177, 322, 137, 161],\\n           [  2,   2,   2,   2],\\n           [407, 379, 536, 492],\\n           [271, 335, 150, 143],\\n           [  2,   1,   2,   2]])\\n    >>> kp_arr.shape\\n    (9, 4)\\n    >>> ds.keypoints.append(kp_arr)\\n    \\n\\nWarning\\n\\nIn order to correctly use the keypoints and connections metadata, it is\\ncritical that all objects in every sample have the same number of K keypoints\\nin the same order. For keypoints that are not present in an image, they can be\\nstored with dummy coordinates of x = 0, y = 0, and v = 0, and the visibility\\nwill prevent them from being drawn in the visualizer.\\n\\n## Point Htype\\uf0c1\\n\\n  * Sample dimensions: `(# points, 2)` in case of 2-D (X, Y) co-ordinates or `(# points, 3)` in case of 3-D (X, Y, Z) co-ordinates of the point.\\n\\nPoints does not contain a fixed mapping across samples between the point order\\nand real-world objects (i.e., point 0 is an elbow, point 1 is a knee, etc.).\\nIf you require such a mapping, use COCO Keypoints Htype.\\n\\n### Creating a point tensor\\uf0c1\\n\\nA point tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"points\", htype=\"point\", sample_compression=None)\\n    \\n\\n  * Optional args:\\n    \\n    * sample_compression or chunk_compression\\n\\n    * dtype: Defaults to `int32`.\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [\"lz4\"]\\n    \\n\\n### Appending point samples\\uf0c1\\n\\n  * Points can be appended as `np.ndarray` or `list`.\\n\\nExamples\\n\\nAppending 2 2-D points\\n\\n    \\n    \\n    >>> ds.points.append([[0, 1], [1, 3]])\\n    \\n\\nAppending 2 3-D points\\n\\n    \\n    \\n    >>> ds.points.append(np.zeros((2, 3)))\\n    \\n\\n## Polygon Htype\\uf0c1\\n\\n  * Sample dimensions: `(# polygons, # points per polygon, # co-ordinates per point)`\\n\\n  * Each sample in a tensor of `polygon` htype is a list of polygons.\\n\\n  * Each polygon is a list / array of points.\\n\\n  * All points in a sample should have the same number of co-ordinates (eg., cannot mix 2-D points with 3-D points).\\n\\n  * Different samples can have different number of polygons.\\n\\n  * Different polygons can have different number of points.\\n\\n### Creating a polygon tensor\\uf0c1\\n\\nA polygon tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"polygons\", htype=\"polygon\", sample_compression=None)\\n    \\n\\n  * Optional args:\\n    \\n    * sample_compression or chunk_compression\\n\\n    * dtype: Defaults to `float32`.\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [\"lz4\"]\\n    \\n\\n### Appending polygons\\uf0c1\\n\\n  * Polygons can be appended as a `list` of `list of tuples` or `np.ndarray`.\\n\\nExamples\\n\\nAppending polygons with 2-D points\\n\\n    \\n    \\n    >>> poly1 = [(1, 2), (2, 3), (3, 4)]\\n    >>> poly2 = [(10, 12), (14, 19)]\\n    >>> poly3 = [(33, 32), (54, 67), (67, 43), (56, 98)]\\n    >>> sample = [poly1, poly2, poly3]\\n    >>> ds.polygons.append(sample)\\n    \\n\\nAppending polygons with 3-D points\\n\\n    \\n    \\n    >>> poly1 = [(10, 2, 9), (12, 3, 8), (12, 10, 4)]\\n    >>> poly2 = [(10, 1, 8), (5, 17, 11)]\\n    >>> poly3 = [(33, 33, 31), (45, 76, 13), (60, 24, 17), (67, 87, 83)]\\n    >>> sample = [poly1, poly2, poly3]\\n    >>> ds.polygons.append(sample)\\n    \\n\\nAppending polygons with numpy arrays\\n\\n    \\n    \\n    >>> import numpy as np\\n    >>> sample = np.random.randint(0, 10, (5, 7, 2))  # 5 polygons with 7 points\\n    >>> ds.polygons.append(sample)\\n    \\n    \\n    \\n    >>> import numpy as np\\n    >>> poly1 = np.random.randint(0, 10, (5, 2))\\n    >>> poly2 = np.random.randint(0, 10, (8, 2))\\n    >>> poly3 = np.random.randint(0, 10, (3, 2))\\n    >>> sample = [poly1, poly2, poly3]\\n    >>> ds.polygons.append(sample)\\n    \\n\\n## Nifti Htype\\uf0c1\\n\\n  * Sample dimensions: `(# height, # width, # slices)` or `(# height, # width, # slices, # time unit)` in case of time-series data.\\n\\n### Creating a nifti tensor\\uf0c1\\n\\nA nifti tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"patients\", htype=\"nifti\", sample_compression=\"nii.gz\")\\n    \\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [\"nii.gz\", \"nii\", None]\\n    \\n\\n### Appending nifti data\\uf0c1\\n\\n  * Nifti samples can be of type `np.ndarray` or `Sample` which is returned by `deeplake.read()`.\\n\\n  * Deep Lake does not support compression of raw nifti data. Therefore, array of raw frames can only be appended to tensors with `None` compression.\\n\\nExamples\\n\\n    \\n    \\n    >>> ds.patients.append(deeplake.read(\"data/patient0.nii.gz\"))\\n    \\n    \\n    \\n    >>> ds.patients.extend([deeplake.read(f\"data/patient{i}.nii.gz\") for i in range(10)])\\n    \\n\\n## Point Cloud Htype\\uf0c1\\n\\n  * Sample dimensions: `(# num_points, 3)`\\n\\n  * Point cloud samples can be of type `np.ndarray` or `Sample` which is returned by `deeplake.read()`.\\n\\n  * Each point cloud is a list / array of points.\\n\\n  * All points in a sample should have the same number of co-ordinates.\\n\\n  * Different point clouds can have different number of points.\\n\\n### Creating a point cloud tensor\\uf0c1\\n\\nA point cloud tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"point_clouds\", htype=\"point_cloud\", sample_compression=\"las\")\\n    \\n\\n  * Optional args:\\n    \\n    * sample_compression\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [None, \"las\"]\\n    \\n\\n### Appending point clouds\\uf0c1\\n\\n  * Point clouds can be appended as a `np.ndarray`.\\n\\nExamples\\n\\nAppending point clouds with numpy arrays\\n\\n    \\n    \\n    >>> import numpy as np\\n    >>> point_cloud1 = np.random.randint(0, 10, (5, 3))\\n    >>> ds.point_clouds.append(point_cloud1)\\n    >>> point_cloud2 = np.random.randint(0, 10, (15, 3))\\n    >>> ds.point_clouds.append(point_cloud2)\\n    >>> ds.point_clouds.shape\\n    >>> (2, None, 3)\\n    \\n\\nOr we can use `deeplake.read()` method to add samples\\n\\n    \\n    \\n    >>> import deeplake as dp\\n    >>> sample = dp.read(\"example.las\") # point cloud with 100 points\\n    >>> ds.point_cloud.append(sample)\\n    >>> ds.point_cloud.shape\\n    >>> (1, 100, 3)\\n    \\n\\n## Mesh Htype\\uf0c1\\n\\n  * Sample dimensions: `(# num_points, 3)`\\n\\n  * Mesh samples can be of type `np.ndarray` or `Sample` which is returned by `deeplake.read()`.\\n\\n  * Each sample in a tensor of `mesh` htype is a mesh array (3-D object data).\\n\\n  * Each mesh is a list / array of points.\\n\\n  * Different meshes can have different number of points.\\n\\n### Creating a mesh tensor\\uf0c1\\n\\nA mesh tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"mesh\", htype=\"mesh\", sample_compression=\"ply\")\\n    \\n\\n  * Optional args:\\n    \\n    * sample_compression\\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [\"ply\"]\\n    \\n\\n### Appending meshes\\uf0c1\\n\\nExamples\\n\\nAppending a ply file containing a mesh data to tensor\\n\\n    \\n    \\n    >>> import deeplake as dp\\n    >>> sample = dp.read(\"example.ply\")  # mesh with 100 points and 200 faces\\n    >>> ds.mesh.append(sample)\\n    \\n    \\n    \\n    >>> ds.mesh.shape\\n    >>> (1, 100, 3)\\n    \\n\\n## Embedding Htype\\uf0c1\\n\\n  * Sample dimensions: `(# elements in the embedding,)`\\n\\n### Creating an embedding tensor\\uf0c1\\n\\nAn embedding tensor can be created using\\n\\n    \\n    \\n    >>> ds.create_tensor(\"embedding\", htype=\"embedding\")\\n    \\n\\n  * Supported compressions:\\n\\n    \\n    \\n    >>> [\"lz4\", None]\\n    \\n\\n### Appending embedding samples\\uf0c1\\n\\n  * Embedding samples can be of type `np.ndarray`.\\n\\nExamples\\n\\nAppending Deep Lake embedding sample\\n\\n    \\n    \\n    >>> ds.embedding.append(np.random.uniform(low=-1, high=1, size=(1024)))\\n    \\n\\nExtending with Deep Lake embeddding samples\\n\\n    \\n    \\n    >>> ds.embedding.extend([np.random.uniform(low=-1, high=1, size=(1024)) for i in range(10)])\\n    \\n\\n## Sequence htype\\uf0c1\\n\\n  * A special meta htype for tensors where each sample is a sequence. The items in the sequence are samples of another htype.\\n\\n  * It is a wrapper htype that can wrap other htypes like `sequence[image]`, `sequence[video]`, `sequence[text]`, etc.\\n\\nExamples\\n\\n    \\n    \\n    >>> ds.create_tensor(\"seq\", htype=\"sequence\")\\n    >>> ds.seq.append([1, 2, 3])\\n    >>> ds.seq.append([4, 5, 6])\\n    >>> ds.seq.numpy()\\n    array([[[1],\\n            [2],\\n            [3]],\\n           [[4],\\n            [5],\\n            [6]]])\\n    \\n    \\n    \\n    >>> ds.create_tensor(\"image_seq\", htype=\"sequence[image]\", sample_compression=\"jpg\")\\n    >>> ds.image_seq.append([deeplake.read(\"img01.jpg\"), deeplake.read(\"img02.jpg\")])\\n    \\n\\n## Link htype\\uf0c1\\n\\n  * Link htype is a special meta htype that allows linking of external data (files) to the dataset, without storing the data in the dataset itself.\\n\\n  * Moreover, there can be variations in this htype, such as `link[image]`, `link[video]`, `link[audio]`, etc. that would enable the activeloop visualizer to correctly display the data.\\n\\n  * No data is actually loaded until you try to read the sample from a dataset.\\n\\n  * There are a few exceptions to this:-\\n    \\n    * If `create_shape_tensor=True` was specified during `create_tensor` of the tensor to which this is being added, the shape of the sample is read. This is `True` by default.\\n\\n    * If `create_sample_info_tensor=True` was specified during `create_tensor` of the tensor to which this is being added, the sample info is read. This is `True` by default.\\n\\n    * If `verify=True` was specified during `create_tensor` of the tensor to which this is being added, some metadata is read from them to verify the integrity of the link samples. This is `True` by default.\\n\\n    * If you do not want to verify your links, all three of `verify`, `create_shape_tensor` and `create_sample_info_tensor` have to be set to `False`.\\n\\nExamples\\n\\n    \\n    \\n    >>> ds = deeplake.dataset(\"......\")\\n    \\n\\nAdding credentials to the dataset\\n\\nYou can add the names of the credentials you want to use (not needed for\\nhttp/local urls)\\n\\n    \\n    \\n    >>> ds.add_creds_key(\"MY_S3_KEY\")\\n    >>> ds.add_creds_key(\"GCS_KEY\")\\n    \\n\\nand populate the added names with credentials dictionaries\\n\\n    \\n    \\n    >>> ds.populate_creds(\"MY_S3_KEY\", {})   # add creds here\\n    >>> ds.populate_creds(\"GCS_KEY\", {})    # add creds here\\n    \\n\\nThese creds are only present temporarily and will have to be repopulated on\\nevery reload.\\n\\nFor datasets connected to Activeloop Platform, you can store your credentials\\non the platform as Managed Credentials and use them just by adding the keys to\\nyour dataset. For example if you have managed credentials with names\\n`\"my_s3_creds\"`, `\"my_gcs_creds\"`, you can add them to your dataset using\\n`Dataset.add_creds_key` without having to populate them.\\n\\n    \\n    \\n    >>> ds.add_creds_key(\"my_s3_creds\", managed=True)\\n    >>> ds.add_creds_key(\"my_gcs_creds\", managed=True)\\n    \\n\\nCreate a link tensor\\n\\n    \\n    \\n    >>> ds.create_tensor(\"img\", htype=\"link[image]\", sample_compression=\"jpg\")\\n    \\n\\nPopulate the tensor with links\\n\\n    \\n    \\n    >>> ds.img.append(deeplake.link(\"s3://abc/def.jpeg\", creds_key=\"my_s3_key\"))\\n    >>> ds.img.append(deeplake.link(\"gcs://ghi/jkl.png\", creds_key=\"GCS_KEY\"))\\n    >>> ds.img.append(deeplake.link(\"https://picsum.photos/200/300\")) # http path doesn’t need creds\\n    >>> ds.img.append(deeplake.link(\"./path/to/cat.jpeg\")) # local path doesn’t need creds\\n    >>> ds.img.append(deeplake.link(\"s3://abc/def.jpeg\"))  # this will throw an exception as cloud paths always need creds_key\\n    :bluebold:`Accessing the data`\\n    \\n    \\n    \\n    >>> for i in range(5):\\n    ...     ds.img[i].numpy()\\n    ...\\n    \\n\\nUpdating a sample\\n\\n    \\n    \\n    >>> ds.img[0] = deeplake.link(\"./data/cat.jpeg\")\\n    \\n\\nPrevious Next\\n\\n* * *\\n\\n(C) Copyright 2022, Activeloop. Revision `db792667`.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n\\nRead the Docs v: latest\\n\\nVersions\\n\\n    latest\\n    v3.1.5\\n    v3.1.0\\n    v3.0.16\\n    v3.0.15\\n    v2.8.5\\n\\nDownloads\\n\\n    pdf\\n    epub\\n\\nOn Read the Docs\\n\\n     Project Home\\n     Builds\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='91fabe16-194c-4551-80c0-14a3e4c1458e', embedding=None, metadata={'source': 'https://docs.deeplake.ai/en/latest/Compressions.html', 'title': 'Compressions — Deep Lake 3.8.20 documentation', 'language': 'en'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nlatest\\n\\nGetting Started\\n\\n  * Installation\\n\\nKey Concepts\\n\\n  * Datasets\\n  * Vector Store\\n  * Tensors\\n  * Htypes\\n  * Compressions\\n    * Sample Compression\\n    * Chunk Compression\\n  * PyTorch and Tensorflow Support\\n  * Utility Functions\\n\\nIntegrations\\n\\n  * Weights and Biases\\n  * MMDetection\\n\\nHigh-Performance Features\\n\\n  * Dataloader\\n  * Sampler\\n  * Tensor Query Language\\n  * Random Split\\n  * Deep Memory\\n\\nAPI Reference\\n\\n  * deeplake\\n  * deeplake.VectorStore\\n  * deeplake.core\\n  * deeplake.core.dataset\\n  * deeplake.core.tensor\\n  * deeplake.api\\n  * deeplake.auto\\n  * deeplake.util\\n  * deeplake.client.log\\n  * deeplake.core.transform\\n  * deeplake.core.vectorstore.deep_memory\\n  * deeplake.random.seed\\n\\n__Deep Lake\\n\\n  * »\\n  * Compressions\\n  * Edit on GitHub\\n\\n* * *\\n\\n# Compressions\\uf0c1\\n\\nDeep Lake can read, compress, decompress and recompress data to different\\nformats. The supported htype-compression configurations are given below.\\n\\nSample Type\\n\\n|\\n\\nHtype\\n\\n|\\n\\nCompressions  \\n  \\n---|---|---  \\n  \\nImage\\n\\n|\\n\\nimage\\n\\n|\\n\\n`bmp`, `dib`, `gif`, `ico`,\\n\\n`jpeg`, `jpeg2000`, `pcx`,\\n\\n`png`, `ppm`, `sgi`, `tga`,\\n\\n`tiff`, `webp`, `wmf`, `xbm`,\\n\\n`eps`, `fli`, `im`, `msp`,\\n\\n`mpo`, `apng`  \\n  \\nVideo\\n\\n|\\n\\nvideo\\n\\n|\\n\\n`mp4`, `mkv`, `avi`  \\n  \\nAudio\\n\\n|\\n\\naudio\\n\\n|\\n\\n`flac`, `mp3`, `wav`  \\n  \\nDicom\\n\\n|\\n\\ndicom\\n\\n|\\n\\n`dcm`  \\n  \\nPoint Cloud\\n\\n|\\n\\npoint_cloud\\n\\n|\\n\\n`las`  \\n  \\nMesh\\n\\n|\\n\\nmesh\\n\\n|\\n\\n`ply`  \\n  \\nOther\\n\\n|\\n\\nbbox, text, list, json, generic, etc.\\n\\n|\\n\\n`lz4`  \\n  \\n## Sample Compression\\uf0c1\\n\\nIf sample compression is specified when `creating tensors`, samples will be\\ncompressed to the given format if possible. If given data is already\\ncompressed and matches the provided `sample_compression`, it will be stored as\\nis. If left as `None`, given samples are uncompressed.\\n\\nNote\\n\\nFor audio and video, we don’t support compressing raw frames but only reading\\ncompressed audio and video data.\\n\\nExamples:\\n\\n    \\n    \\n    >>> ds.create_tensor(\"images\", htype=\"image\", sample_compression=\"jpg\")\\n    \\n    >>> ds.create_tensor(\"videos\", htype=\"video\", sample_compression=\"mp4\")\\n    \\n    >>> ds.create_tensor(\"point_clouds\", htype=\"point_cloud\", sample_compression=\"las\")\\n    \\n\\nStructure of sample-wise compressed tensor.\\uf0c1\\n\\n## Chunk Compression\\uf0c1\\n\\nIf chunk compression is specified when `creating tensors`, added samples will\\nbe clubbed together and compressed to the given format chunk-wise. If given\\ndata is already compressed, it will be uncompressed and then recompressed\\nchunk-wise.\\n\\nNote\\n\\nChunk-wise compression is not supported for audio, video and point_cloud\\nhtypes.\\n\\nExamples:\\n\\n    \\n    \\n    >>> ds.create_tensor(\"images\", htype=\"image\", chunk_compression=\"jpg\")\\n    \\n    >>> ds.create_tensor(\"boxes\", htype=\"bbox\", chunk_compression=\"lz4\")\\n    \\n\\nStructure of chunk-wise compressed tensor.\\uf0c1\\n\\nNote\\n\\nSee `deeplake.read()` to learn how to read data from files and populate these\\ntensors.\\n\\nPrevious Next\\n\\n* * *\\n\\n(C) Copyright 2022, Activeloop. Revision `db792667`.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n\\nRead the Docs v: latest\\n\\nVersions\\n\\n    latest\\n    v3.1.5\\n    v3.1.0\\n    v3.0.16\\n    v3.0.15\\n    v2.8.5\\n\\nDownloads\\n\\n    pdf\\n    epub\\n\\nOn Read the Docs\\n\\n     Project Home\\n     Builds\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52db8f30-6d0b-4a68-b8b2-e555d9348992', embedding=None, metadata={'source': 'https://docs.deeplake.ai/en/latest/Pytorch-and-Tensorflow-Support.html', 'title': 'PyTorch and Tensorflow Support — Deep Lake 3.8.20 documentation', 'language': 'en'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nlatest\\n\\nGetting Started\\n\\n  * Installation\\n\\nKey Concepts\\n\\n  * Datasets\\n  * Vector Store\\n  * Tensors\\n  * Htypes\\n  * Compressions\\n  * PyTorch and Tensorflow Support\\n  * Utility Functions\\n\\nIntegrations\\n\\n  * Weights and Biases\\n  * MMDetection\\n\\nHigh-Performance Features\\n\\n  * Dataloader\\n  * Sampler\\n  * Tensor Query Language\\n  * Random Split\\n  * Deep Memory\\n\\nAPI Reference\\n\\n  * deeplake\\n  * deeplake.VectorStore\\n  * deeplake.core\\n  * deeplake.core.dataset\\n  * deeplake.core.tensor\\n  * deeplake.api\\n  * deeplake.auto\\n  * deeplake.util\\n  * deeplake.client.log\\n  * deeplake.core.transform\\n  * deeplake.core.vectorstore.deep_memory\\n  * deeplake.random.seed\\n\\n__Deep Lake\\n\\n  * »\\n  * PyTorch and Tensorflow Support\\n  * Edit on GitHub\\n\\n* * *\\n\\n# PyTorch and Tensorflow Support\\uf0c1\\n\\nDeep Lake datasets can be easily converted to Torch dataloaders or Tensorflow\\ndatasets for training.\\n\\n`Dataset.pytorch`\\n\\n|\\n\\nConverts the dataset into a pytorch Dataloader.  \\n  \\n---|---  \\n  \\n`Dataset.tensorflow`\\n\\n|\\n\\nConverts the dataset into a tensorflow compatible format.  \\n  \\nPrevious Next\\n\\n* * *\\n\\n(C) Copyright 2022, Activeloop. Revision `db792667`.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n\\nRead the Docs v: latest\\n\\nVersions\\n\\n    latest\\n    v3.1.5\\n    v3.1.0\\n    v3.0.16\\n    v3.0.15\\n    v2.8.5\\n\\nDownloads\\n\\n    pdf\\n    epub\\n\\nOn Read the Docs\\n\\n     Project Home\\n     Builds\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f04e2c73-4201-4d54-afa0-4b35248d8954', embedding=None, metadata={'source': 'https://docs.deeplake.ai/en/latest/Utility-Functions.html', 'title': 'Utility Functions — Deep Lake 3.8.20 documentation', 'language': 'en'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nlatest\\n\\nGetting Started\\n\\n  * Installation\\n\\nKey Concepts\\n\\n  * Datasets\\n  * Vector Store\\n  * Tensors\\n  * Htypes\\n  * Compressions\\n  * PyTorch and Tensorflow Support\\n  * Utility Functions\\n    * General Functions\\n    * Making Deep Lake Samples\\n    * Parallelism\\n\\nIntegrations\\n\\n  * Weights and Biases\\n  * MMDetection\\n\\nHigh-Performance Features\\n\\n  * Dataloader\\n  * Sampler\\n  * Tensor Query Language\\n  * Random Split\\n  * Deep Memory\\n\\nAPI Reference\\n\\n  * deeplake\\n  * deeplake.VectorStore\\n  * deeplake.core\\n  * deeplake.core.dataset\\n  * deeplake.core.tensor\\n  * deeplake.api\\n  * deeplake.auto\\n  * deeplake.util\\n  * deeplake.client.log\\n  * deeplake.core.transform\\n  * deeplake.core.vectorstore.deep_memory\\n  * deeplake.random.seed\\n\\n__Deep Lake\\n\\n  * »\\n  * Utility Functions\\n  * Edit on GitHub\\n\\n* * *\\n\\n# Utility Functions\\uf0c1\\n\\n## General Functions\\uf0c1\\n\\n`exists`\\n\\n|\\n\\nChecks if a dataset exists at the given `path`.  \\n  \\n---|---  \\n  \\n## Making Deep Lake Samples\\uf0c1\\n\\n`read`\\n\\n|\\n\\nUtility that reads raw data from supported files into Deep Lake format.  \\n  \\n---|---  \\n  \\n`link`\\n\\n|\\n\\nUtility that stores a link to raw data.  \\n  \\n`link_tiled`\\n\\n|\\n\\nUtility that stores links to multiple images that act as tiles and together\\nform a big image.  \\n  \\n## Parallelism\\uf0c1\\n\\n`compute`\\n\\n|\\n\\nCompute is a decorator for functions.  \\n  \\n---|---  \\n  \\n`compose`\\n\\n|\\n\\nTakes a list of functions decorated using `deeplake.compute()` and creates a\\npipeline that can be evaluated using .eval  \\n  \\nTransform pipelines returned by `compute()` and `compose()` are evaluated\\nusing `eval`:\\n\\n`eval`\\n\\n|\\n\\nEvaluates the pipeline on `data_in` to produce an output dataset `ds_out`.  \\n  \\n---|---  \\n  \\nPrevious Next\\n\\n* * *\\n\\n(C) Copyright 2022, Activeloop. Revision `db792667`.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n\\nRead the Docs v: latest\\n\\nVersions\\n\\n    latest\\n    v3.1.5\\n    v3.1.0\\n    v3.0.16\\n    v3.0.15\\n    v2.8.5\\n\\nDownloads\\n\\n    pdf\\n    epub\\n\\nOn Read the Docs\\n\\n     Project Home\\n     Builds\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f7639c1e-cc28-4128-bdeb-8f2cbc8a5699', embedding=None, metadata={'source': 'https://docs.deeplake.ai/en/latest/Weights-and-Biases.html', 'title': 'Weights and Biases — Deep Lake 3.8.20 documentation', 'language': 'en'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nlatest\\n\\nGetting Started\\n\\n  * Installation\\n\\nKey Concepts\\n\\n  * Datasets\\n  * Vector Store\\n  * Tensors\\n  * Htypes\\n  * Compressions\\n  * PyTorch and Tensorflow Support\\n  * Utility Functions\\n\\nIntegrations\\n\\n  * Weights and Biases\\n    * Logging Dataset Creation\\n    * Logging Dataset Read\\n  * MMDetection\\n\\nHigh-Performance Features\\n\\n  * Dataloader\\n  * Sampler\\n  * Tensor Query Language\\n  * Random Split\\n  * Deep Memory\\n\\nAPI Reference\\n\\n  * deeplake\\n  * deeplake.VectorStore\\n  * deeplake.core\\n  * deeplake.core.dataset\\n  * deeplake.core.tensor\\n  * deeplake.api\\n  * deeplake.auto\\n  * deeplake.util\\n  * deeplake.client.log\\n  * deeplake.core.transform\\n  * deeplake.core.vectorstore.deep_memory\\n  * deeplake.random.seed\\n\\n__Deep Lake\\n\\n  * »\\n  * Weights and Biases\\n  * Edit on GitHub\\n\\n* * *\\n\\n# Weights and Biases\\uf0c1\\n\\nDeep Lake’s Weights and Biases integration allows you to track and improve\\nreproducibility of your machine learning experiments. Deep Lake will\\nautomatically push all information required to reproduce the snapshot of the\\ndata like your dataset’s URI, commit ID, and view IDs of any views that you\\nhave used in your training workflow.\\n\\nLearn more about Weights and Biases here.\\n\\n## Logging Dataset Creation\\uf0c1\\n\\nIf you create a Deep Lake dataset using any of the functions mentioned in\\nCreating Datasets, just perform a commit on the dataset to log its creation on\\nW&B.\\n\\n    \\n    \\n    >>> run = wandb.init(project=\"deeplake_wandb\", job_type=\"dataset_upload\")\\n    >>> ds = deeplake.empty(\"hub://fayazrahman4u/my_dataset\") # create dataset\\n    >>> ds.create_tensor(\"images\", htype=\"image\", sample_compression=\"jpg\") # create a tensor\\n    >>> ds.images.append(deeplake.read(\"files/images/dog.jpg\")) # add a sample\\n    >>> ds.commit(\"creation\") # commit -> trigger logging\\n    >>> run.finish()\\n    \\n\\nNote\\n\\nIf you created your dataset using `deeplake.deepcopy()`, perform the commit\\nonly if you have head changes.\\n\\nNote\\n\\nIf you make changes to an existing dataset, commit the changes with an active\\nWeights and Biases run to log it’s state.\\n\\n## Logging Dataset Read\\uf0c1\\n\\nA dataset read will be logged if you iterate over a dataset or call\\n`Dataset.pytorch()` or `Tensor.numpy()` on its tensors.\\n\\n    \\n    \\n    >>> run = wandb.init(project=\"deeplake_wandb\", job_type=\"torch dataloader\")\\n    >>> train_loader = ds.pytorch()\\n    >>> run.finish()\\n    \\n    \\n    \\n    >>> run = wandb.init(project=\"deeplake_wandb\", job_type=\"iteration\")\\n    >>> for sample in ds:\\n    >>>     print(sample[\"images\"].shape)\\n    >>> run.finish()\\n    \\n\\nPrevious Next\\n\\n* * *\\n\\n(C) Copyright 2022, Activeloop. Revision `db792667`.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n\\nRead the Docs v: latest\\n\\nVersions\\n\\n    latest\\n    v3.1.5\\n    v3.1.0\\n    v3.0.16\\n    v3.0.15\\n    v2.8.5\\n\\nDownloads\\n\\n    pdf\\n    epub\\n\\nOn Read the Docs\\n\\n     Project Home\\n     Builds\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_documents(url):\n",
    "    all_links = get_all_links(url)\n",
    "    all_links=all_links[:10]\n",
    "    loader = AsyncHtmlLoader(all_links)\n",
    "    docs = loader.load()\n",
    "\n",
    "    html2text = Html2TextTransformer()\n",
    "    docs_transformed = html2text.transform_documents(docs)\n",
    "    docs = [Document.from_langchain_format(doc) for doc in docs_transformed]\n",
    "    return docs\n",
    "\n",
    "\n",
    "docs = load_documents(\"https://docs.deeplake.ai/en/latest/\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "vector_store = DeepLakeVectorStore(\n",
    "    dataset_path=\"hub://srishtysuman2919/deeplake_docs_deepmemory2\",\n",
    "    overwrite=False,  # set to True to overwrite the existing dataset\n",
    "    runtime={\"tensor_db\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_storage_and_service_contexts( vector_store, docs=[], populate_vector_store=True):\n",
    "    if populate_vector_store:\n",
    "        node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
    "        nodes = node_parser.get_nodes_from_documents(docs)\n",
    "    else:\n",
    "        nodes = []\n",
    "\n",
    "    # by default, the node ids are set to random uuids. To ensure same id's per run, we manually set them.\n",
    "    for idx, node in enumerate(nodes):\n",
    "        node.id_ = f\"node_{idx}\"\n",
    "\n",
    "    llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    return service_context, storage_context, nodes, llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context, storage_context, nodes, llm = create_storage_and_service_contexts( docs=docs, vector_store=vector_store, populate_vector_store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to deeplake dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:04<00:00, 11.26it/s]\n",
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://srishtysuman2919/deeplake_docs_deepmemory2', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      "   text       text      (55, 1)      str     None   \n",
      " metadata     json      (55, 1)      str     None   \n",
      " embedding  embedding  (55, 1536)  float32   None   \n",
      "    id        text      (55, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex( nodes, service_context=service_context, storage_context=storage_context)\n",
    "deep_memory_retriever = vector_index.as_retriever(similarity_top_k=4, deep_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training Deep Memory\n",
    "    \n",
    "    We need relevance, queries together with corpus data (data that we want to query). The corpus data was already populated in the previous section; here, we will be generating questions and relevance.\n",
    "\n",
    "    questions - is a text of strings, where each string represents a query.\n",
    "\n",
    "    relevance - contains links to the ground truth for each question. There might be several docs that contain an answer to the given question. Because of this, relevance is List[List[tuple[str, float]]], where the outer list represents queries and the inner list relevant documents. The tuple contains a str, float pair where the string represents the id of the source doc (corresponds to the id tensor in the dataset), while the float corresponds to how much the current document is related to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_datasets( number_of_samples=600, llm=None, nodes=None, save=False):\n",
    "    random_indices = random.sample(range(len(nodes)), number_of_samples)\n",
    "\n",
    "    ratio = int(len(random_indices) * 0.8)\n",
    "\n",
    "    train_indices = random_indices[:ratio]\n",
    "    test_indices = random_indices[ratio:]\n",
    "\n",
    "    train_nodes = [nodes[i] for i in train_indices]\n",
    "    test_nodes = [nodes[i] for i in test_indices]\n",
    "\n",
    "    train_qa_dataset = generate_question_context_pairs(train_nodes, llm=llm, num_questions_per_chunk=1)\n",
    "\n",
    "    test_qa_dataset = generate_question_context_pairs(test_nodes, llm=llm, num_questions_per_chunk=1)\n",
    "\n",
    "    # [optional] save\n",
    "    if save:\n",
    "        train_qa_dataset.save_json(f\"deeplake_docs_{number_of_samples}_train.json\")\n",
    "        test_qa_dataset.save_json(f\"deeplake_docs_{number_of_samples}_test.json\")\n",
    "    return train_qa_dataset, test_qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:42<00:00, 12.83s/it]\n",
      "100%|██████████| 2/2 [00:46<00:00, 23.27s/it]\n"
     ]
    }
   ],
   "source": [
    "train_qa_dataset, test_qa_dataset = create_train_test_datasets(number_of_samples=10, llm=llm, nodes=nodes, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"deeplake_docs_10_train.json\")\n",
    "test_qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"deeplake_docs_10_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_relevance(qa_dataset):\n",
    "    \"\"\"Function for converting llama-index dataset to correct format for deep memory training\"\"\"\n",
    "    queries = [text for _, text in qa_dataset.queries.items()]\n",
    "    relevant_docs = qa_dataset.relevant_docs\n",
    "    relevance = []\n",
    "    for doc in relevant_docs:\n",
    "        relevance.append([(relevant_docs[doc][0], 1)])\n",
    "    return queries, relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries, train_relevance = create_query_relevance(train_qa_dataset)\n",
    "test_queries, test_relevance = create_query_relevance(test_qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DeepMemory training job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data for deepmemory:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 85 embeddings in 1 batches of size 85:: 100%|██████████| 1/1 [00:22<00:00, 22.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepMemory training job started. Job ID: 65c5d4b03ccbda4c0dba81ed\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "job_id = vector_store._vectorstore.deep_memory.train(\n",
    "    queries=train_queries,\n",
    "    relevance=train_relevance,\n",
    "    embedding_function=embeddings.embed_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DeepMemory Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = vector_store.vectorstore.deep_memory.evaluate(\n",
    "    queries=test_queries,\n",
    "    relevance=test_relevance,\n",
    "    embedding_function=embeddings.embed_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RetrieverEvaluator to examine the MRR (Mean Reciprocal Rank) and hit rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "    hit_rates = []\n",
    "    mrrs = []\n",
    "    names = []\n",
    "    for name, eval_result in eval_results.items():\n",
    "        metric_dicts = []\n",
    "        for er in eval_result:\n",
    "            metric_dict = er.metric_vals_dict\n",
    "            metric_dicts.append(metric_dict)\n",
    "\n",
    "        full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "        hit_rate = full_df[\"hit_rate\"].mean()\n",
    "        mrr = full_df[\"mrr\"].mean()\n",
    "\n",
    "        hit_rates.append(hit_rate)\n",
    "        mrrs.append(mrr)\n",
    "        names.append(name)\n",
    "\n",
    "    metric_df = pd.DataFrame(\n",
    "        [\n",
    "            {\"retrievers\": names[i], \"hit_rate\": hit_rates[i], \"mrr\": mrrs[i]}\n",
    "            for i in range(2)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating performance of retrieval with deep memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "deep_memory_retriever = vector_index.as_retriever( similarity_top_k=10, vector_store_kwargs={\"deep_memory\": True})\n",
    "dm_retriever_evaluator = RetrieverEvaluator.from_metric_names([\"mrr\", \"hit_rate\"], retriever=deep_memory_retriever)\n",
    "\n",
    "dm_eval_results = await dm_retriever_evaluator.aevaluate_dataset(test_qa_dataset, retriever=dm_retriever_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_retriever = vector_index.as_retriever(similarity_top_k=10)\n",
    "naive_retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=naive_retriever\n",
    ")\n",
    "\n",
    "naive_eval_results = await naive_retriever_evaluator.aevaluate_dataset(\n",
    "    test_qa_dataset, retriever=naive_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>with with Deep Memory top-10 eval</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.129347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>without with Deep Memory top-10 eval</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.407197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             retrievers  hit_rate       mrr\n",
       "0     with with Deep Memory top-10 eval  0.363636  0.129347\n",
       "1  without with Deep Memory top-10 eval  0.909091  0.407197"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = {\n",
    "    f\"{mode} with Deep Memory top-10 eval\": eval_result\n",
    "    for mode, eval_result in zip(\n",
    "        [\"with\", \"without\"], [dm_eval_results, naive_eval_results]\n",
    "    )\n",
    "}\n",
    "\n",
    "display_results(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Memory Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can connect your own storage to the deeplake by using the Vector Store feature, which allows you to store and access your data from any storage provider. You can also use the deeplake API to connect to your own storage and access your data.\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine(vector_store_kwargs={\"deep_memory\": True})\n",
    "response = query_engine.query(\n",
    "    \"How can you connect your own storage to the deeplake?\"\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can connect your own storage to deeplake by using the deeplake.VectorStore class, which allows you to create a custom vector store that can be used to store and retrieve data from your own storage system. This can be done by implementing the necessary methods and interfaces in your custom vector store class.\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine(\n",
    "    vector_store_kwargs={\"deep_memory\": False}\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"How can you connect your own storage to the deeplake?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
