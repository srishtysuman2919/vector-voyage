{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 75042  100 75042    0     0   134k      0 --:--:-- --:--:-- --:--:--  135k\n"
     ]
    }
   ],
   "source": [
    "# ! mkdir -p 'data/srishty/'\n",
    "# ! curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/srishty/srishty_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('/Users/srishtysuman/.env')\n",
    "print(load_dotenv('/Users/srishtysuman/.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index import VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "import deeplake\n",
    "from openai import OpenAI\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Llama-index nodes/chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SimpleDirectoryReader: Load files from file directory and Automatically select the best file reader given file extensions.\n",
    "documents = SimpleDirectoryReader(\"./data/srishty/\").load_data()\n",
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleNodeParser deprecated by SentenceSplitter: Parse text with a preference for complete sentences.\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
    "# node_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_nodes_from_documents: Parse documents into nodes.\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "# nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, the node/chunks ids are set to random uuids. To ensure same id's per run, we manually set them.\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node_{idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 1\n",
      "Number of nodes: 57 with the current chunk size of 512\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Documents: {len(documents)}\")\n",
    "print(f\"Number of nodes: {len(nodes)} with the current chunk size of {node_parser.chunk_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a local Deep Lake vector store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create a DeepLakeVectorStore locally to store the vectors\n",
    "dataset_path = \"./data/srishty/deep_lake_db\"\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM that will answer questions with the retrieved context\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "embed_model = OpenAIEmbedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srishtysuman/anaconda3/envs/langchain/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating embeddings: 100%|██████████| 57/57 [00:02<00:00, 21.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to deeplake dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 443.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./data/srishty/deep_lake_db', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      "   text       text      (57, 1)      str     None   \n",
      " metadata     json      (57, 1)      str     None   \n",
      " embedding  embedding  (57, 1536)  float32   None   \n",
      "    id        text      (57, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex(nodes, service_context=service_context, storage_context=storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the local Vectore Store to Activeloop's platform and convert it into a managed database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = \"./data/srishty/deep_lake_db\"\n",
    "hub_path = \"hub://srishtysuman2919/optimization_srishty\"\n",
    "hub_managed_path = \"hub://srishtysuman2919/optimization_srishty_managed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying dataset: 96%|█████████▋| 27/28 [00:44<00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/srishtysuman2919/optimization_srishty\n",
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(path='hub://srishtysuman2919/optimization_srishty', tensors=['embedding', 'id', 'metadata', 'text'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First upload our local vector store\n",
    "deeplake.deepcopy(local, hub_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/srishtysuman2919/optimization_srishty\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://srishtysuman2919/optimization_srishty loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "ds = deeplake.load('hub://srishtysuman2919/optimization_srishty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying dataset: 96%|█████████▋| 27/28 [01:07<00:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/srishtysuman2919/optimization_srishty_managed\n",
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(path='hub://srishtysuman2919/optimization_srishty_managed', tensors=['embedding', 'id', 'metadata', 'text'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a managed vector store under a different name\n",
    "deeplake.deepcopy(hub_path, hub_managed_path, overwrite=True, runtime={\"tensor_db\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a Vector Store with the managed dataset that we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://srishtysuman2919/optimization_srishty_managed already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "db = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, read_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching our docs and ids from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "# Fetch dataset docs and ids \n",
    "docs = db._vectorstore.dataset.text.data(fetch_chunks=True, aslist=True)['value']\n",
    "ids = db._vectorstore.dataset.id.data(fetch_chunks=True, aslist=True)['value']\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a synthetic training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def generate_question(text):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a world class expert for generating questions based on provided context. \\\n",
    "                        You make sure the question can be answered by the text.\"},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text,\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except:\n",
    "        question_string = \"No question generated\"\n",
    "        return question_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(docs: list[str], ids: list[str], n: int):\n",
    "    questions = []\n",
    "    relevances = []\n",
    "    pbar = tqdm(total=n)\n",
    "    while len(questions) < n:\n",
    "        # 1. randomly draw a piece of text and relevance id\n",
    "        r = random.randint(0, len(docs)-1)\n",
    "        text, label = docs[r], ids[r]\n",
    "\n",
    "        # 2. generate queries and assign and relevance id\n",
    "        generated_qs = [generate_question(text)]\n",
    "        if generated_qs == [\"No question generated\"]:\n",
    "            print(\"No question generated\")\n",
    "            continue\n",
    "\n",
    "        questions.extend(generated_qs)\n",
    "        relevances.extend([[(label, 1)] for _ in generated_qs])\n",
    "        pbar.update(len(generated_qs))\n",
    "\n",
    "    return questions[:n], relevances[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the query generation process with a desired size of 40 queries/questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [07:55<04:58, 21.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [14:00<00:00, 21.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "What was the distinctive thing about Y Combinator (YC) and how did the founders come up with the batch model concept?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions, relevances = generate_queries(docs, ids, n=40)\n",
    "print(len(questions)) \n",
    "print(questions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Deep Memory Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DeepMemory training job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srishtysuman/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data for deepmemory:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 40 embeddings in 1 batches of size 40:: 100%|██████████| 1/1 [00:20<00:00, 20.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepMemory training job started. Job ID: 65c5c5503ccbda4c0dba81e8\n"
     ]
    }
   ],
   "source": [
    "openai_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "job_id = db._vectorstore.deep_memory.train(\n",
    "    queries=questions,\n",
    "    relevance=relevances,\n",
    "    embedding_function=openai_embeddings.embed_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/srishtysuman2919/optimization_srishty_managed\n",
      "--------------------------------------------------------------\n",
      "|                  65c5c5503ccbda4c0dba81e8                  |\n",
      "--------------------------------------------------------------\n",
      "| status                     | pending                       |\n",
      "--------------------------------------------------------------\n",
      "| progress                   | None                          |\n",
      "--------------------------------------------------------------\n",
      "| results                    | not available yet             |\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# During training you can check the status of the training run\n",
    "db._vectorstore.deep_memory.status(job_id=job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a Deep Memory-enabled inference by setting deep_memory=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main things Paul worked on before college?\"\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "db = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, read_only=True,)\n",
    "vector_index = VectorStoreIndex.from_vector_store(db, service_context=service_context, storage_context=storage_context, show_progress=True)\n",
    "\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=3, vector_store_kwargs={\"deep_memory\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul worked on several different things before college, including writing essays, launching a software project, and working on an online store builder.\n"
     ]
    }
   ],
   "source": [
    "response_vector = query_engine.query(query)\n",
    "print(response_vector.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's run a quantitative evaluation on another set of synthetically generated test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [08:35<04:13, 21.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [12:46<16:29, 89.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [19:41<31:16, 187.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [34:18<59:10, 394.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [41:28<54:00, 405.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [48:44<48:19, 414.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [55:57<41:59, 419.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [1:03:11<35:21, 424.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [1:10:08<28:08, 422.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [1:17:23<21:17, 425.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [1:24:38<14:17, 428.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [1:31:52<07:10, 430.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n",
      "No question generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [1:39:10<00:00, 148.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding queries took 2.21 seconds\n",
      "---- Evaluating without Deep Memory ---- \n",
      "Recall@1:\t  55.0%\n",
      "Recall@3:\t  72.5%\n",
      "Recall@5:\t  77.5%\n",
      "Recall@10:\t  90.0%\n",
      "Recall@50:\t  100.0%\n",
      "Recall@100:\t  100.0%\n",
      "---- Evaluating with Deep Memory ---- \n",
      "Recall@1:\t  42.5%\n",
      "Recall@3:\t  60.0%\n",
      "Recall@5:\t  70.0%\n",
      "Recall@10:\t  85.0%\n",
      "Recall@50:\t  100.0%\n",
      "Recall@100:\t  100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'with model': {'recall@1': 0.425,\n",
       "  'recall@3': 0.6,\n",
       "  'recall@5': 0.7,\n",
       "  'recall@10': 0.85,\n",
       "  'recall@50': 1.0,\n",
       "  'recall@100': 1.0},\n",
       " 'without model': {'recall@1': 0.55,\n",
       "  'recall@3': 0.725,\n",
       "  'recall@5': 0.775,\n",
       "  'recall@10': 0.9,\n",
       "  'recall@50': 1.0,\n",
       "  'recall@100': 1.0}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate validation queries\n",
    "validation_questions, validation_relevances = generate_queries(docs, ids, n=40)\n",
    "\n",
    "# Launch the evaluation function\n",
    "recalls = db._vectorstore.deep_memory.evaluate(\n",
    "    queries=validation_questions,\n",
    "    relevance=validation_relevances,\n",
    "    embedding_function=openai_embeddings.embed_documents,\n",
    ")\n",
    "recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
